{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "parsing_spacy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/iamrajee/Data-Analysis/blob/master/parsing_spacy.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "75WPGlb2nApM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://spacy.io/usage/linguistic-features\n",
        "\n",
        "https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa"
      ]
    },
    {
      "metadata": {
        "id": "s2gySQQVjGxE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "collapsed": true,
        "outputId": "81c7db17-b6bf-4c51-a94f-1d9b21cac0e8"
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.0.11)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.28.0)\n",
            "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.3)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (2017.4.5)\n",
            "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.31.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.7.1)\n",
            "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.10.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (4.23.4)\n",
            "Requirement already satisfied: msgpack-python in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.5.6)\n",
            "Requirement already satisfied: msgpack-numpy==0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.4.1)\n",
            "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.8.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.1.0)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.11.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.10.11)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy) (0.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V83M74xijKVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f92f964e-8eb5-4710-b336-6fa7a822ad14"
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\r\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fh7viaR7jQXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4695
        },
        "collapsed": true,
        "outputId": "04e46937-0b0d-4801-aeb7-89499aaabc6e"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /content/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /content/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /content/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /content/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /content/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /content/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /content/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /content/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /content/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /content/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /content/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /content/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /content/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /content/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /content/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /content/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /content/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /content/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /content/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /content/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /content/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /content/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /content/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /content/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /content/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /content/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /content/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /content/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /content/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /content/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /content/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /content/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /content/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /content/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /content/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /content/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /content/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /content/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /content/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /content/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /content/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /content/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /content/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /content/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /content/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /content/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "GZ6B_RXbjbBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !python -m spacy.en.download all\n",
        "# # nlp = spacy.load(“en”)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qdDNMNOji5tP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# doc = \"Dependency parsing with ID/LP rules is trivial if you have a lexicon. One uses a (declarative) generate-and-test approach. Dependency trees are rooted spanning trees on a graph with n nodes where n is the length of the processed sentence. One can declare what a tree is and - now comes the linguistic part - where an edge (that is, a dependency) may be. The result are all spanning trees composed of the possible dependencies. Thus dependency parsing is constraint solving.\"\n",
        "# document = doc\n",
        "# # extract all review sentences that contains the term - hotel\n",
        "# hotel = [sent for sent in document.sents if 'hotel' in sent.string.lower()]\n",
        "\n",
        "\n",
        "# create dependency tree\n",
        "# sentence = hotel[2] \n",
        "# sentence = \"A naïve implementation of the algorithm would have a terrible time complexity.\"\n",
        "# for word in sentence:\n",
        "#   print(str(list(word.children)))\n",
        "#   #print(word + ': ' + str(list(word.children))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eNHK1hT9rDqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "collapsed": true,
        "outputId": "ba22769d-943f-4970-fa94-1ca9b7832750"
      },
      "cell_type": "code",
      "source": [
        "!python3 -m spacy download en"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /usr/local/lib/python3.6/dist-packages\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "whFSsCq8nbYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "288216ff-49f4-4609-f1cb-83170dca8cb5"
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from nltk import Tree\n",
        "\n",
        "doc1 = \"Dependency parsing with ID/LP rules is trivial if you have a lexicon. One uses a (declarative) generate-and-test approach. Dependency trees are rooted spanning trees on a graph with n nodes where n is the length of the processed sentence. One can declare what a tree is and - now comes the linguistic part - where an edge (that is, a dependency) may be. The result are all spanning trees composed of the possible dependencies. Thus dependency parsing is constraint solving.\"\n",
        "doc2 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "f = open('1.txt', 'r')\n",
        "doc3 = str(f.read())\n",
        "en_nlp = spacy.load('en')\n",
        "\n",
        "doc = en_nlp(doc2)\n",
        "# print(doc)\n",
        "# def to_nltk_tree(node):\n",
        "#     if node.n_lefts + node.n_rights > 0:\n",
        "#         return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "#     else:\n",
        "#         return node.orth_\n",
        "\n",
        "      \n",
        "def tok_format(tok):\n",
        "    return \"_\".join([tok.orth_, tok.tag_])\n",
        "\n",
        "\n",
        "def to_nltk_tree(node):\n",
        "    if node.n_lefts + node.n_rights > 0:\n",
        "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
        "    else:\n",
        "        return tok_format(node)      \n",
        "      \n",
        "\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "[print(to_nltk_tree(sent.root)) for sent in doc.sents]\n",
        "\n",
        "\n",
        "# for sent in doc.sents:\n",
        "#   root_token = sent.root\n",
        "#   for child in root_token.children:\n",
        "#     if child.dep_ == 'nsubj':\n",
        "#       subj = child\n",
        "#     if child.dep_ == 'dobj':\n",
        "#       obj = child\n",
        "# #     print(child)\n",
        "# #     print(child.dep)\n",
        "      \n",
        "# print(subj)\n",
        "# print(obj)\n",
        " "
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           jumps_VBZ                                \n",
            "  _____________|________________________             \n",
            " |             |                     over_IN        \n",
            " |             |                        |            \n",
            " |           fox_NN                   dog_NN        \n",
            " |     ________|________         _______|_______     \n",
            "._. The_DT  quick_JJ brown_JJ the_DT         lazy_JJ\n",
            "\n",
            "(jumps_VBZ\n",
            "  (fox_NN The_DT quick_JJ brown_JJ)\n",
            "  (over_IN (dog_NN the_DT lazy_JJ))\n",
            "  ._.)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "metadata": {
        "id": "WMQCYhlJtPz7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrXXcas5qk5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fefc5e30-c926-4ea4-ab0c-6c5e973b5664"
      },
      "cell_type": "code",
      "source": [
        "spacy.explain(\"NN\")"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'noun, singular or mass'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "metadata": {
        "id": "-t9QkBx4GR48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "outputId": "145df7ab-007d-4fff-ad6d-6cc16bce1a33"
      },
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1625\" height=\"399.5\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">quick</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">brown</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">fox</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">jumps</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">over</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">lazy</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">dog.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,2.0 575.0,2.0 575.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,266.5 L923.0,254.5 907.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M945,264.5 C945,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "HG0OGR0AMs5t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# spacy.load('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9XF_yksTHp5u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "7b1625d6-598b-41e7-9636-d1902c31c66a"
      },
      "cell_type": "code",
      "source": [
        "doc = en_nlp(u'This is a sentence.')\n",
        "# displacy.serve(doc, style='dep')\n",
        "options = {'compact': True, 'bg': '#09a3d5',\n",
        "           'color': 'white', 'font': 'Source Sans Pro'}\n",
        "displacy.render(doc, style='dep', options=options, jupyter = True)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"650\" height=\"287.0\" style=\"max-width: none; height: 287.0px; color: white; background: #09a3d5; font-family: Source Sans Pro\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">sentence.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,127.0 197.0,127.0 197.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M362,152.0 362,127.0 497.0,127.0 497.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M362,154.0 L358,146.0 366,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M212,152.0 212,102.0 500.0,102.0 500.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M500.0,154.0 L504.0,146.0 496.0,146.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gs2iPrhOMLGX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !python3 -m spacy download custom_ner_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUHaJyTMIJ90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "34b506fa-9be4-40e8-aa71-11747f8c9ce7"
      },
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "text = \"\"\"But Google is starting from behind. The company made a late push\n",
        "into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa\n",
        "software, which runs on its Echo and Dot devices, have clear leads in\n",
        "consumer adoption.\"\"\"\n",
        "\n",
        "# nlp = spacy.load('custom_ner_model')\n",
        "doc = en_nlp(text)\n",
        "displacy.render(doc, style='ent', jupyter = True)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5\">But \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is starting from behind. The company made a late push\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    \n",
              "\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "into hardware, and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "’s Siri, available on \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    iPhones\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              ", and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Amazon\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "’s \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Alexa\n",
              "\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "software, which runs on its \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Echo\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Dot\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " devices, have clear leads in\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    \n",
              "\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "consumer adoption.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AGyPC8gbMEtg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ab673771-b681-4a22-fe62-613a64f7b25a"
      },
      "cell_type": "code",
      "source": [
        "#finding noun chunk\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "doc = en_nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "          chunk.root.head.text)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Autonomous cars cars nsubj shift\n",
            "insurance liability liability dobj shift\n",
            "manufacturers manufacturers pobj toward\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zw13sCwXj7Pc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a04a47c9-9adc-4ae1-d257-c17f4a5cc24a"
      },
      "cell_type": "code",
      "source": [
        "#exploring parse tree\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        "          [child for child in token.children])"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Autonomous amod cars NOUN []\n",
            "cars nsubj shift VERB [Autonomous]\n",
            "shift ROOT shift VERB [cars, liability, toward]\n",
            "insurance compound liability NOUN []\n",
            "liability dobj shift VERB [insurance]\n",
            "toward prep shift VERB [manufacturers]\n",
            "manufacturers pobj toward ADP []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fo_1038flq47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72f0a57e-6e09-4a26-a4bd-6a5d315456df"
      },
      "cell_type": "code",
      "source": [
        "#finding subject in sentence\n",
        "\n",
        "verbs = set()\n",
        "for possible_subject in doc:\n",
        "    if possible_subject.dep == 426 and possible_subject.head.pos == 99:\n",
        "        verbs.add(possible_subject.head)\n",
        "print(verbs)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{shift}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p4pUQZ_wmlWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b53a284c-838b-4bcc-8dab-71b08d4e1581"
      },
      "cell_type": "code",
      "source": [
        "# Finding a verb with a subject from above — less good\n",
        "verbs = []\n",
        "for possible_verb in doc:\n",
        "    if possible_verb.pos == 99:\n",
        "        for possible_subject in possible_verb.children:\n",
        "            if possible_subject.dep == 426:\n",
        "                verbs.append(possible_verb)\n",
        "                break\n",
        "                \n",
        "                \n",
        "print(verbs)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[shift]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Z7YOGveoQja",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a1954ece-2ca6-4eb9-e795-60468105dc3c"
      },
      "cell_type": "code",
      "source": [
        "#iterating aroung local tree\n",
        "\n",
        "doc = en_nlp(u\"bright red apples on the tree\")\n",
        "# doc1 = \"Dependency parsing with ID/LP rules is trivial if you have a lexicon. One uses a (declarative) generate-and-test approach. Dependency trees are rooted spanning trees on a graph with n nodes where n is the length of the processed sentence. One can declare what a tree is and - now comes the linguistic part - where an edge (that is, a dependency) may be. The result are all spanning trees composed of the possible dependencies. Thus dependency parsing is constraint solving.\"\n",
        "# doc = en_nlp(doc1)\n",
        "# # doc = en_nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "print([token.text for token in doc[2].lefts])  # ['bright', 'red']\n",
        "print([token.text for token in doc[2].rights])  # ['on']\n",
        "print(doc[2].n_lefts)  # 2\n",
        "print(doc[2].n_rights)  # 1"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          apples_NNS        \n",
            "     _________|_________     \n",
            "    |         |       on_IN \n",
            "    |         |         |    \n",
            "    |         |      tree_NN\n",
            "    |         |         |    \n",
            "bright_JJ   red_JJ    the_DT\n",
            "\n",
            "['bright', 'red']\n",
            "['on']\n",
            "2\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xDKbtBoIpV5m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # nlp = spacy.load('de_core_news_sm')\n",
        "# doc = nlp(u\"schöne rote Äpfel auf dem Baum\")\n",
        "# print([token.text for token in doc[2].lefts])  # ['schöne', 'rote']\n",
        "# print([token.text for token in doc[2].rights])  # ['auf']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgRDrdLopoEZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "083101a2-b8ab-48c2-ef50-90971f8d2535"
      },
      "cell_type": "code",
      "source": [
        "# subtree, ancestors\n",
        "\n",
        "doc = en_nlp(u\"Credit and mortgage account holders must submit their requests\")\n",
        "# doc = en_nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "root = [token for token in doc if token.head == token][0]\n",
        "subject = list(root.lefts)[0]\n",
        "for descendant in subject.subtree:\n",
        "    assert subject is descendant or subject.is_ancestor(descendant)\n",
        "    print(descendant.text, descendant.dep_, descendant.n_lefts,\n",
        "          descendant.n_rights,\n",
        "          [ancestor.text for ancestor in descendant.ancestors])"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        submit_VB                                     \n",
            "    ________|__________________________________        \n",
            "   |              holders_NNS                  |      \n",
            "   |                   |                       |       \n",
            "   |               Credit_NN                   |      \n",
            "   |         __________|___________            |       \n",
            "   |        |                  account_NN requests_NNS\n",
            "   |        |                      |           |       \n",
            "must_MD   and_CC              mortgage_NN  their_PRP$ \n",
            "\n",
            "Credit nmod 0 2 ['holders', 'submit']\n",
            "and cc 0 0 ['Credit', 'holders', 'submit']\n",
            "mortgage compound 0 0 ['account', 'Credit', 'holders', 'submit']\n",
            "account conj 1 0 ['Credit', 'holders', 'submit']\n",
            "holders nsubj 1 0 ['submit']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ImVImM61qr8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#if want dont want parser\n",
        "\n",
        "\n",
        "\n",
        "# nlp = spacy.load('en', disable=['parser'])\n",
        "# doc = nlp(u\"I don't want parsed\", disable=['parser'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmYWJLUvs2oh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "c26d981a-827f-4a25-e3d7-f15879650c3c"
      },
      "cell_type": "code",
      "source": [
        "#merging,   .left_edge and .right_edge\n",
        "\n",
        "\n",
        "print(doc[4])\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
        "\n",
        "\n",
        "print('\\n\\n\\n')\n",
        "span = doc[doc[4].left_edge.i : doc[4].right_edge.i+1]\n",
        "span.merge()\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_, token.head.text)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "holders\n",
            "        submit_VB                                     \n",
            "    ________|__________________________________        \n",
            "   |              holders_NNS                  |      \n",
            "   |                   |                       |       \n",
            "   |               Credit_NN                   |      \n",
            "   |         __________|___________            |       \n",
            "   |        |                  account_NN requests_NNS\n",
            "   |        |                      |           |       \n",
            "must_MD   and_CC              mortgage_NN  their_PRP$ \n",
            "\n",
            "Credit NOUN nmod holders\n",
            "and CCONJ cc Credit\n",
            "mortgage NOUN compound account\n",
            "account NOUN conj Credit\n",
            "holders NOUN nsubj submit\n",
            "must VERB aux submit\n",
            "submit VERB ROOT submit\n",
            "their ADJ poss requests\n",
            "requests NOUN dobj submit\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                 submit_VB             \n",
            "        _____________|__________        \n",
            "       |             |     requests_NNS\n",
            "       |             |          |       \n",
            "Credit and mortg  must_MD   their_PRP$ \n",
            "  age account                          \n",
            "   holders_NN                          \n",
            "\n",
            "Credit and mortgage account holders NOUN nsubj submit\n",
            "must VERB aux submit\n",
            "submit VERB ROOT submit\n",
            "their ADJ poss requests\n",
            "requests NOUN dobj submit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_cFTh5MEs_G3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3760
        },
        "outputId": "a5862435-8210-40da-dbe3-3df8e9fbd388"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "doc = en_nlp(doc3)\n",
        "# print(doc)\n",
        "Label = []\n",
        "Labels = {}\n",
        "for ent in doc.ents:\n",
        "#     print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
        "  Label.append(ent.label_)  \n",
        "#   Labels[ent.label_] = ent.text\n",
        "#   print(ent.text, ent.label_)\n",
        "\n",
        "Label = np.unique(Label)\n",
        "n_labels = len(Label)\n",
        "print(n_labels)\n",
        "print(Label)\n",
        "# print(Labels)\n",
        "# for i,ele in enumerate(Labels):\n",
        "#   print(i)\n",
        "\n",
        "entlist = []\n",
        "for i in range(0,n_labels):\n",
        "  entlist.append(i)\n",
        "  entlist[i] = []\n",
        "temp = []\n",
        "for ent in doc.ents:\n",
        "  for i in range(0,n_labels):\n",
        "    if ent.label_ == Label[i]:\n",
        "      entlist[i].append(ent.text)\n",
        "      break;\n",
        "   \n",
        "  \n",
        "for i in range(0,n_labels):\n",
        "  entlist[i] = np.unique(entlist[i])\n",
        "  \n",
        "  \n",
        "for i in range(0,n_labels):\n",
        "  print('\\n' + Label[i] + ':')\n",
        "  print(entlist[i])"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n",
            "['CARDINAL' 'DATE' 'EVENT' 'FAC' 'GPE' 'LOC' 'MONEY' 'NORP' 'ORDINAL'\n",
            " 'ORG' 'PERCENT' 'PERSON' 'PRODUCT' 'QUANTITY' 'TIME' 'WORK_OF_ART']\n",
            "\n",
            "CARDINAL:\n",
            "['\\n' '\\n2' '1' '10' '100' '106' '1078' '108' '1085–1103' '109' '11' '112'\n",
            " '1123–1137' '1137–1176' '114' '12' '122–141' '13' '132–143.2013' '14'\n",
            " '15' '1543–1567' '16' '17' '175' '182–205' '183–206' '188' '19' '190'\n",
            " '1–23' '2' '20' '2000' '2007)—are' '206' '21' '217–244' '221–235' '239'\n",
            " '24' '240' '247' '25' '255–' '25–32' '260' '267' '267–268' '268' '28'\n",
            " '288' '29' '299' '3' '30' '306' '31' '310' '311' '315–386' '316'\n",
            " '317–342' '32' '324' '334–357' '337–365' '34' '342–369' '345' '35' '350'\n",
            " '355' '357' '35–59' '36' '363–383' '37' '37–' '38' '385–' '4' '40' '41'\n",
            " '421– 449' '425' '429' '43' '430' '433– 448' '435' '459' '46' '47' '470'\n",
            " '471' '473' '474' '48' '481' '49' '497' '5' '50' '51' '539' '56'\n",
            " '577–598' '59 –\\n100' '6' '60' '619 – 644' '62' '644 – 675' '65' '658'\n",
            " '67' '7' '70 – 82' '740' '76' '77(4' '79' '8' '80' '81' '82' '827– 832'\n",
            " '82–111' '82–98' '83' '83–100' '84' '844' '85–97' '86' '86 – 87' '87'\n",
            " '88' '9' '90' '91' '92' '94' '95' '96' '97)—is' 'About half' 'Four'\n",
            " 'MM-5' 'One' 'Only one' 'Ten' 'Three' 'Two' 'about 500' 'at least one'\n",
            " 'eight' 'five' 'four' 'help340' 'just under half' 'more than\\n10' 'one'\n",
            " 'three' 'tions' 'two']\n",
            "\n",
            "DATE:\n",
            "['& Day' '16(1' '1939' '1960' '1963' '1964' '1966' '1967' '1973' '1975'\n",
            " '1977' '1978' '1979' '1983' '1986' '1987' '1988' '1989' '1990' '1991'\n",
            " '1992' '1993' '1994' '1995' '1996' '1997' '1998' '1999' '2000' '2001'\n",
            " '2002' '2003' '2004' '2005' '2006' '2007' '2008' '2009' '2010' '2013'\n",
            " '45' '5– 48' '61' 'A couple of days later' 'A. 1960' 'A. 1963' 'A. 1967'\n",
            " 'A. 1987' 'A. 2009' 'Annual' 'April' 'April\\n' 'B. 2000' 'Christmas'\n",
            " 'D. 2000' 'H. 1973' 'H. 1992' 'J. 1979' 'J. 1988' 'J. 1995' 'J. 2006'\n",
            " 'J. 2009' 'J. M. 2000' 'K. 1991' 'K. 1994' 'L. 1977' 'L. 1999' 'L. 2004'\n",
            " 'M. 1998' 'M. 2010' 'Mondays' 'P. 2003' 'P. 2009' 'Quarterly' 'R. 2002'\n",
            " 'S. 1999' 'S. 2002' 'S. 2005' 'S. 2006' 'S. 2010' 'Stinglhamber'\n",
            " 'Three months ago' 'W. 1939' 'W. 2001' 'each week' 'four-year' 'monthly'\n",
            " 'next\\nmonth' 'the day' 'the de-\\n' 'the first half of the'\n",
            " 'the months of January and September' 'the twentieth century' 'today'\n",
            " 'two years' 'weekly']\n",
            "\n",
            "EVENT:\n",
            "['\\nContinuing'\n",
            " '\\nFIGURE 4\\nEmergent Model of How Two Patterns of Managerial Helping Relate'\n",
            " '\\nPh.D.' '\\nX\\nX\\nX\\nX\\nX\\nX\\nX\\nX\\nX\\n' '\\nwork”' 'DISCUSSION\\n'\n",
            " 'Reframing\\nX\\nX\\nTransforming\\n' 'all”\\n(E-6)' 'the\\nstory”'\n",
            " 'this.’”\\nManagers' '”\\n']\n",
            "\n",
            "FAC:\n",
            "['\\nDark Side\\nManagers\\nDisappointment' 'Discrepant\\n'\n",
            " 'Ego-Strengthening\\n' 'Gould-\\n' 'Helping\\n' 'L.\\nBerkowitz'\n",
            " 'Positive\\nShortcut' 'tea.’\\n']\n",
            "\n",
            "GPE:\n",
            "['\\n' '\\nAmanatullah' '\\nFrost' '\\nS.' '\\nYork:' '& Beckman' '& Griffin'\n",
            " 'A. S.' 'Amanatullah' 'Anand' 'Armeli' 'Ashkanasy' 'Bacharach'\n",
            " 'Bamberger' 'Barsade' 'Bergeron' 'Blau' 'Bulletin' 'C.' 'CA' 'CT'\n",
            " 'Cambridge' 'Chicago' 'Cotterell' 'Dirsmith' 'Dutton' 'Elfenbein'\n",
            " 'Gender' 'Gerstner' 'Gower' 'HLM-1' 'Hareli' 'Human Performance' 'Jung'\n",
            " 'K.' 'Krone' 'L.' 'Latané' 'Lexington' 'Littlefield' 'London' 'M. 2007'\n",
            " 'MD' 'Mintzberg' 'New York' 'New York:' 'OCB' 'OUTCOMES' 'Organ'\n",
            " 'Ostroff' 'Outcomes' 'Palo\\nAlto' 'Palo Alto' 'Pearce' 'Rafaeli' 'Review'\n",
            " 'S.' 'San Francisco' 'Stamford' 'Sutton' 'Thousand Oaks'\n",
            " 'Transformational' 'U.S.' 'UK' 'US' 'Wayne' 'Whiting']\n",
            "\n",
            "LOC:\n",
            "['Eds' 'Middle level']\n",
            "\n",
            "MONEY:\n",
            "['215–\\n285' '234 –\\n261' '334\\n' '349\\n' '466 –\\n485' 'roughly 10']\n",
            "\n",
            "NORP:\n",
            "['African-American' 'Armeli' 'Bedeian' 'Chinese' 'Christian' 'D.' 'Hareli'\n",
            " 'Heian' 'Interviews' 'Karagonlar' 'L.' 'MM-1' 'R.' 'Reagans' 'Reviews'\n",
            " 'Thoits' '–123' '–205']\n",
            "\n",
            "ORDINAL:\n",
            "['16th' 'First' 'first' 'fourth' 'second' 'third']\n",
            "\n",
            "ORG:\n",
            "['\\nAcademy of Management Journal\\n' '\\nEmotions' '\\nHowell & Frost'\n",
            " '\\nInstrumental\\n' '\\nLexington Books' '\\nManagers' '\\nOffers to\\n'\n",
            " '\\nReciprocation\\n' '\\nRecognition' '\\nResearch Question 2'\n",
            " '\\nSpector & Fox' '\\nSubordinates\\nPerception' '\\nTheory' '& Ames'\n",
            " '& Brass' '& Darby' '& Fox' '& Frost' '& Ilies' '& Jennings' '& Legendre'\n",
            " '& Morgan' '& Mossholder' '& Perrewé' '& Rafaeli' '& Samuel'\n",
            " '& Sivasubramaniam' '& Speicher' '& Tripoli' '& Tse' '& Weber' '.43'\n",
            " '493–504.356\\nAcademy of Management Journal\\n'\n",
            " 'A. P.\\nBrief & B. M. Staw' 'A., & Hunt' 'Academy\\nof Management Journal'\n",
            " 'Academy\\nof Management Review' 'Academy of\\nManagement Review'\n",
            " 'Academy of Manage-\\n' 'Academy of Management\\nJournal'\n",
            " 'Academy of Management Journal' 'Academy of Management Journal\\n'\n",
            " 'Admin-\\n' 'Administrative Science\\nQuarterly' 'Advances' 'Aldine'\n",
            " 'American\\nJournal of Sociology' 'American Journal\\nof Sociology'\n",
            " 'American Journal of Soci-\\n' 'American Sociological Review'\n",
            " 'Analytic Dimensions\\n' 'Anand\\n' 'Anand\\nSpitzmuller' 'Anand\\nal'\n",
            " 'Anand, &\\nKilduff' 'Ashkanasy & Tse' 'Avolio & Bass' 'Avolio, B. J.'\n",
            " 'Avolio, B. J., & Bass' 'Avolio, Walumbwa, &\\nWeber' 'B. 2007'\n",
            " 'B., & Arrowood' 'B., & Strauss' 'Bacharach, S.'\n",
            " 'Bamberger, & Sonnenstuhl' 'Barling, J., Weber'\n",
            " 'Barling, Weber, & Kelloway' 'Baron' 'Bedeian &\\nHunt'\n",
            " 'Berkeley: University\\nof California Press.' 'Bolino & Turnley'\n",
            " 'Bolino, M. C., & Turnley' 'Bowler & Brass' 'Brescoll & Uhlmann'\n",
            " 'C. Cooper &\\n' 'C. Cooper & J. Barling' 'Cameron' 'Cialdini'\n",
            " 'Community Mental Health Journal' 'Contribution to Theory and Research\\n'\n",
            " 'Copyright of the Academy of Management' 'Cornell University'\n",
            " 'Cornell University Press' 'Cummings' 'Cummings, & McLean Parks,350\\n'\n",
            " 'Discrepant\\n' 'Discrepant Expectations' 'Dutton, & Quinn' 'E-11' 'E-12'\n",
            " 'E-14' 'E-7' 'E., & Thye, S. 1999' 'E4' 'Edwards' 'Effectiveness'\n",
            " 'Eisenberger et\\n' 'Eisenberger, & Speicher' 'Eisenhardt & Graeb-\\n'\n",
            " 'Elfen-\\n' 'Employee Rights and' 'Employees’ Negative Emotions'\n",
            " 'F., & Dickson' 'F., & Jennings' 'F., Neves,\\nP., Becker' 'Fineman'\n",
            " 'First-Order Analysis' 'Frost' 'Frost & Robinson' 'George & Brief'\n",
            " 'George, J. M., & Brief' 'Gioia & Chittipeddi' 'Glaser & Strauss'\n",
            " 'Goldstein & Goldberg' 'Gonzalez-Morales' 'Gouldner' 'Grandey'\n",
            " 'Grandey, A. 2003' 'Grandey, A. 2008' 'Grandey, A. A. 2000' 'Guilford'\n",
            " 'HarperCollins' 'Harvard Business' 'Harvard Business School Press'\n",
            " 'Harvard University Press' 'Helping\\n' 'Holt Rinehart & Winston' 'Howell'\n",
            " 'Human Decision Processes' 'Human Resource' 'IMD, Lausanne'\n",
            " 'IMD, Lausanne\\nMARTIN KILDUFF\\nUniversity College London\\n'\n",
            " 'Impression\\n'\n",
            " 'Incongruent Expectations Concerning Reciprocity for Emotion Help\\n'\n",
            " 'Informant\\nAnalytic Dimensions\\n' 'Inhib-\\n' 'J. J.\\nGross'\n",
            " 'J. P.\\nWalsh & A. P. Brief' 'J., & Quinn' 'J., & Swartz'\n",
            " 'J., Worline, M., & Wilson, A. 2000' 'Journal'\n",
            " 'Journal\\nof Applied Psychology' 'Journal of\\nApplied Psychology'\n",
            " 'Journal of\\nBehavioral Decision Making' 'Journal of\\nClassification'\n",
            " 'Journal of Applied Psy-\\n' 'Journal of Applied Psychology'\n",
            " 'Journal of Occupational' 'Journal of Occupational Health Psychology'\n",
            " 'Journal of Personality' 'Journal of Personality and\\nSocial Psychology'\n",
            " 'Journal of Personality and Social\\nPsychology' 'K. S. Cook & J. Hagan'\n",
            " 'K., & Graebner' 'Kahn, W. A. 1993' 'Kilduff' 'Koehler'\n",
            " 'Kroeck, & Sivasubramaniam, 1996' 'Krone &\\nMorgan, 2000'\n",
            " 'L. L. Cummings & B. M. Staw\\n' 'L., & Ellis' 'L., & Ilies' 'L., & Liden'\n",
            " 'LMX' 'LMX\\n' 'Latané & Arro-\\n' 'Lawler' 'Lawler & Thye'\n",
            " 'Leipzig University' 'Listening\\n' 'Listening\\n1\\n' 'London & New York:'\n",
            " 'London Business School' 'Looking\\n' 'M., & Huberman, A. 1994'\n",
            " 'M., & Steiger-\\nMueller' 'MLQ' 'MM-2' 'Mahwah, NJ: Erlbaum' 'Manag-\\n'\n",
            " 'Management' 'Management\\n' 'Management Executive' 'Management Journal'\n",
            " 'Managerial Behavior\\n' 'Managerial Rank a\\nRank n Mean s.d'\n",
            " 'Managers\\nSignificant' 'Martin, J., Knopoff' 'Metric' 'Miles & Huberman'\n",
            " 'Mintzberg' 'Morrison' 'Morrison, E. 1994' 'Mutual'\n",
            " 'N. ANAND\\nIMD, Lausanne\\n' 'N. Ashkanasy & C. Hartel & W. Zerbe'\n",
            " 'N., & Kilduff' 'NY' 'New\\nYork:' 'OTREG' 'Oldham &\\nBrass'\n",
            " 'Oldham & Brass' 'Organizational' 'Organizational Behavior\\n'\n",
            " 'Organizational Psy-\\n' 'Outcomes of\\nDiscrepant Expectations\\n'\n",
            " 'Oxford University\\nPress' 'P.' 'P. M., &\\nBlume' 'P., & Lynch'\n",
            " 'P., & Robinson' 'P., & Sonnenstuhl' 'P., & Vashdi' 'PI-1' 'Pantheon'\n",
            " 'Podsakoff, &\\nBlume' 'Porter, & Tripoli' 'Positive\\n'\n",
            " 'Psychological\\nScience' 'Relational correlates\\n' 'Respondents'\n",
            " 'Richards' 'Rimé' 'Roethlisberger & Dickson' 'S.\\nFineman'\n",
            " 'S., & Chittipeddi' 'S., & Rafaeli, A. 2008' 'S., & Yoon'\n",
            " 'Sally Maitlis\\n' 'Scans\\n' 'Science' 'Scott & J. Blake'\n",
            " 'Settoon & Mossholder' 'Shell' 'Shore, &\\nLiden' 'Simon & Nath'\n",
            " 'So-\\ncial Psychology' 'Social commit-\\n' 'Status'\n",
            " 'Strategic\\nManagement Journal' 'Subordinates' 'Subordinates\\nDistrust'\n",
            " 'Subordinates\\nEmotion' 'Subordinates\\nWorry' 'Summary of Results\\n'\n",
            " 'Surface\\n' 'T., & Kelloway' 'THE SERVICE SECTOR\\n' 'TOEGEL\\n'\n",
            " 'Techniques\\n' 'The atmo-\\n' 'The door-2013\\nToegel' 'Turner'\n",
            " 'Types of Emotion Help' 'University College London'\n",
            " 'University of Chicago Press' 'Validating\\n' 'Van\\nMaanen, 1988'\n",
            " 'Van Dyne' 'Van Dyne & Ellis' 'Vanderbilt University' 'Vol'\n",
            " 'Wheeler, & Darby' 'Williams & Swartz' 'Worline, & Wil-\\n'\n",
            " 'Yammarino, & Fleenor' 'Yammarino, F., & Fleenor' 'Zellars' 'agers’'\n",
            " 'emo-342\\nAcademy of Management Journal\\nTABLE 1\\nExtent of Homophily'\n",
            " 'employ-\\n' 'figures—\\n' 'managers348\\n' 'managers’' 'managers’ part\\n'\n",
            " 'posi-344\\nAcademy of Management Journal\\n' 'previously352\\n'\n",
            " 'providing2013' 'smile’' 'the Academy of Man-\\n'\n",
            " 'the University of\\nCambridge' 'the University of Alberta'\n",
            " 'the University of Cambridge' 'those2013\\nToegel' 'viewed2013\\nToegel'\n",
            " '–563' '”\\n' '” (Frost\\n& Robinson']\n",
            "\n",
            "PERCENT:\n",
            "['10%' '100%' '11\\npercent' '11 percent' '18%' '27%' '36%' '45 percent'\n",
            " '71\\npercent' '75 percent' '9%' 'Approximately 70 percent'\n",
            " 'Sixty percent' 'close to 40\\npercent' 'less than 5 percent']\n",
            "\n",
            "PERSON:\n",
            "['\\nToegel' '\\nVashdi' '& Berson' '& Chittipeddi' '& Goldberg' '& Kahn'\n",
            " '& LePine' '& McLean Parks' '& N. M. Ashkanasy' '& Uhlmann' 'A. E.'\n",
            " 'A. M.\\n' 'A. P. 1992' 'A. R. 1983' 'Avolio' 'B. D. 2009' 'B. J.'\n",
            " 'B. L. 1975' 'B. M.' 'B. M. 1995' 'Bacharach' 'Bamberger' 'Bass'\n",
            " 'Bergeron' 'Bolino' 'Bolton' 'Bowler' 'C. 1998' 'C. E. J. Hartel'\n",
            " 'C. S. 2002' 'CA' 'Catalan' 'Catalan, J.,\\nWheeler' 'Covaleski' 'D. I.'\n",
            " 'D. J. 1979' 'D. J. 2006' 'D. L. Schacter' 'D. V. 1997' 'D. W. 1988'\n",
            " 'D. W. 1997' 'De Botton' 'Dirsmith' 'Dobbin' 'E.' 'E. K. 1996'\n",
            " 'E. L. 2008' 'E. T.' 'Ed' 'Eds' 'F. J.' 'F. O.' 'Fasolo' 'Flynn' 'Frost'\n",
            " 'G.' 'G. K.' 'G. R.' 'Ginka Toegel' 'Gioia' 'Glaser' 'Goldstein'\n",
            " 'Grandey' 'H. A. 2007' 'HLM-7' 'Handbook' 'Hugh Willmott' 'Humphrey'\n",
            " 'Ithaca' 'J.' 'J.\\n' 'J. A. 1998' 'J. A. M.\\nCoyle-Shapiro' 'J. B.'\n",
            " 'J. B. 2004' 'J. Barling' 'J. C.,' 'J. E., Lewis' 'J. L.' 'J. M.' 'Jung'\n",
            " 'K. B.' 'K. J.' 'K. L.' 'K. W. 2002' 'Kahn' 'Kahn\\n' 'Katz' 'Knopoff'\n",
            " 'Kroeck' 'L. L.' 'L. M. Shore' 'L. V.' 'L. W.' 'Lanham' 'Lawler'\n",
            " 'Leadership Quarterly' 'Lewis' 'Lowe' 'M. A.' 'M. A. 2006' 'M. C. 1999'\n",
            " 'M. S. Taylor' 'M. W.' 'M., Van Dyne' 'MM-1' 'MM-4' 'Martin'\n",
            " 'Martin Kilduff' 'Martin et al' 'Meyerson' 'N. M.,' 'N. M., & Daus'\n",
            " 'N. P.' 'N., Eisenberger' 'NVivo' 'Oldham' 'P. A. 1989' 'P. E.'\n",
            " 'P. J. 1989' 'P. L. 2001' 'Pearce' 'Peter Bamberger' 'Podsakoff' 'Porter'\n",
            " 'Q. N. 2005' 'R. 2008' 'R. B.' 'R. E.' 'R. H.' 'R. I.' 'R. L. 1966'\n",
            " 'R. P.' 'Reframing' 'Richards' 'Ross' 'Rousseau' 'S.' 'S. E.\\nTaylor'\n",
            " 'S. Fineman' 'S. K.' 'S. T. 2004' 'S. T. Fiske' 'S. W.' 'S., Eisenberger'\n",
            " 'S., Shore' 'Simon' 'Spitzmuller' 'Spradley' 'Sutton' 'T.' 'T. J. 2009'\n",
            " 'Thaler' 'Thomas' 'Thomas, J., Clark' 'Thye' 'Toegel' 'Tsui' 'V. L.'\n",
            " 'Van Maanen' 'Vincent' 'W. H. 2005' 'W. J. 1996' 'W. J. Zerbe' 'W. M.'\n",
            " 'W. N.' 'W. R.' 'Walumbwa' 'Williams' 'Y.' 'al' 'anand@imd.ch'\n",
            " 'individuals’' 'ployees’' '⫹1' 'ⴚ.03']\n",
            "\n",
            "PRODUCT:\n",
            "['Administrative Science Quarterly' 'Podsakoff' 'Science Quarterly'\n",
            " 'Table 1' 'Table 2' 'Toegel']\n",
            "\n",
            "QUANTITY:\n",
            "['335\\n' '347\\n' '351\\n' '353\\n' '46 .43']\n",
            "\n",
            "TIME:\n",
            "['343\\n' 'a couple of minutes' 'about an hour' 'one midranking']\n",
            "\n",
            "WORK_OF_ART:\n",
            "['\\n(Grandey, 2000:' 'Ph.D.' 'Rafferty' 'Refining definitions\\n'\n",
            " 'the Provision of Emotion Help\\n' '“Is\\n' '” (' '” (E-2)' '” (E-9)'\n",
            " '” (HLM-2)' '” (HLM-7)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kdaGfH8rw2ss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "020569bb-bb3d-4eb2-f121-dfd92a5d23b2"
      },
      "cell_type": "code",
      "source": [
        "#adding costume boundaries\n",
        "\n",
        "text = u\"this is a sentence...hello...and another sentence.\"\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "doc = en_nlp(text)\n",
        "print('Before:', [sent.text for sent in doc.sents])\n",
        "\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == '...':\n",
        "            doc[token.i+1].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "# en_nlp.add_pipe(set_custom_boundaries, before='parser')\n",
        "doc = en_nlp(text)\n",
        "print('After:', [sent.text for sent in doc.sents])"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before: ['this is a sentence...', 'hello...and another sentence.']\n",
            "After: ['this is a sentence...', 'hello...and another sentence.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jG3J3NZSHVCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "73581f99-b1a6-4765-aab0-5f1a3d733344"
      },
      "cell_type": "code",
      "source": [
        "# import spacy\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "matcher = Matcher(en_nlp.vocab)\n",
        "matched_sents = [] # collect data of matched sentences to be visualized\n",
        "\n",
        "def collect_sents(matcher, doc, i, matches):\n",
        "    match_id, start, end = matches[i]\n",
        "    span = doc[start : end]  # matched span\n",
        "    sent = span.sent  # sentence containing matched span\n",
        "    # append mock entity for match in displaCy style to matched_sents\n",
        "    # get the match span by ofsetting the start and end of the span with the\n",
        "    # start and end of the sentence in the doc\n",
        "    match_ents = [{'start': span.start_char - sent.start_char,\n",
        "                   'end': span.end_char - sent.start_char,\n",
        "                   'label': 'MATCH'}]\n",
        "    matched_sents.append({'text': sent.text, 'ents': match_ents })\n",
        "\n",
        "pattern = [{'LOWER': 'facebook'}, {'LEMMA': 'be'}, {'POS': 'ADV', 'OP': '*'},\n",
        "           {'POS': 'ADJ'}]\n",
        "matcher.add('FacebookIs', collect_sents, pattern)  # add pattern\n",
        "doc = en_nlp(u\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "# serve visualization of sentences containing match with displaCy\n",
        "# set manual=True to make displaCy render straight from a dictionary\n",
        "# (if you're not running the code within a Jupyer environment, you can\n",
        "# remove jupyter=True and use displacy.serve instead)\n",
        "displacy.render(matched_sents, style='ent', manual=True, jupyter=True)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5\">I'd say that \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Facebook is evil\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\n",
              "</mark>\n",
              ".</div>\n",
              "\n",
              "<div class=\"entities\" style=\"line-height: 2.5\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Facebook is pretty cool\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\n",
              "</mark>\n",
              ", right?</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "9JaBhdKULvf8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ca5efa32-899a-48df-9065-98e2ef75849a"
      },
      "cell_type": "code",
      "source": [
        "ex = [{'text': 'But Google is starting from behind.',\n",
        "       'ents': [{'start': 4, 'end': 10, 'label': 'ORG'}],\n",
        "       'title': None}]\n",
        "html = displacy.render(ex, style='ent', manual=True)\n",
        "displacy.render(ex, style='ent', manual=True, jupyter = True)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5\">But \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is starting from behind.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BCZC9weTQmfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "71a32a72-960e-40e7-e2ee-e5f81f4e8f62"
      },
      "cell_type": "code",
      "source": [
        "print(html)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<div class=\"entities\" style=\"line-height: 2.5\">But \n",
            "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
            "    Google\n",
            "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
            "</mark>\n",
            " is starting from behind.</div>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ODqp8t4lPrLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1771f3af-8ea1-4ce9-f3fa-e519322948e4"
      },
      "cell_type": "code",
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(html))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5\">But \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is starting from behind.</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sDlLZbFKRuui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "57efb247-bca6-432e-ef85-d6d685876566"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.txt\t nltk_data     This-is-an-example.svg\r\n",
            "datalab  sentence.svg  This-is-another-one.svg\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X4gCvUFUPuH9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5b46a77-2d53-4a0c-926f-b4e3a44f1582"
      },
      "cell_type": "code",
      "source": [
        "#saving files\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "sentences = [\"This is an example.\", \"This is another one.\"]\n",
        "for sent in sentences:\n",
        "    doc = en_nlp(sent)\n",
        "    svg = displacy.render(doc, style='dep')\n",
        "    file_name = '-'.join([w.text for w in doc if not w.is_punct]) + '.svg'\n",
        "    output_path = Path(file_name)\n",
        "    output_path.open('w', encoding='utf-8').write(svg)\n",
        "    \n",
        "svg = displacy.render(doc, style='dep')\n",
        "output_path = Path('sentence.svg')\n",
        "output_path.open('w', encoding='utf-8').write(svg)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "metadata": {
        "id": "dUKIfhb1Q6g1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c23973c2-cfb7-4113-a9c3-0a98a847f468"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.txt\t nltk_data     This-is-an-example.svg\r\n",
            "datalab  sentence.svg  This-is-another-one.svg\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SBYbI57SR6aE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1295
        },
        "outputId": "96fa1caa-4068-49db-9b78-c16cfcd63e98"
      },
      "cell_type": "code",
      "source": [
        "doc = en_nlp(doc1)\n",
        "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
        "[to_nltk_tree(sent.root) for sent in doc.sents]"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                             is_VBZ                                            \n",
            "       ________________________|_______________________________                 \n",
            "      |           |       |         parsing_VBG                |               \n",
            "      |           |       |              |                     |                \n",
            "      |           |       |           with_IN                  |               \n",
            "      |           |       |              |                     |                \n",
            "      |           |       |          rules_NNS              have_VBP           \n",
            "      |           |       |              |               ______|_________       \n",
            "      |           |       |            LP_NNP           |      |     lexicon_NN\n",
            "      |           |       |     _________|________      |      |         |      \n",
            "Dependency_NN trivial_JJ ._. ID_NNP             /_SYM if_IN you_PRP     a_DT   \n",
            "\n",
            "                            uses_VBZ                                          \n",
            "    ___________________________|_____________                                  \n",
            "   |     |                              approach_NN                           \n",
            "   |     |    _______________________________|__________________               \n",
            "   |     |   |      |          |             |             generate_VB        \n",
            "   |     |   |      |          |             |         _________|_________     \n",
            "   |     |   |      |          |             |        |         |      test_NN\n",
            "   |     |   |      |          |             |        |         |         |    \n",
            "One_PRP ._. a_DT (_-LRB- declarative_JJ   )_-RRB-   -_HYPH    and_CC    -_HYPH\n",
            "\n",
            "                                         are_VBP                                                                              \n",
            "  __________________________________________|_________________________________________                                         \n",
            " |        |                  rooted_VBN                                             is_VBZ                                    \n",
            " |        |                      |                                      ______________|________                                \n",
            " |        |                 spanning_VBG                               |                   length_NN                          \n",
            " |        |            __________|__________________                   |        _______________|__________                     \n",
            " |        |           |        on_IN                |                  |       |                        of_IN                 \n",
            " |        |           |          |                  |                  |       |                          |                    \n",
            " |    trees_NNS       |       graph_NN           with_IN           where_WRB   |                     sentence_NN              \n",
            " |        |           |          |           _______|________          |       |                __________|____________        \n",
            "._. Dependency_NN trees_NNS     a_DT       n_CC          nodes_NNS    n_CC   the_DT          the_DT              processed_VBN\n",
            "\n",
            "                                                                     declare_VB                                                                                   \n",
            "    _____________________________________________________________________|_________________________                                                                \n",
            "   |      |     |            |                                                                 comes_VBZ                                                          \n",
            "   |      |     |            |                      _______________________________________________|________                                                       \n",
            "   |      |     |            |                     |                                                     part_NN                                                  \n",
            "   |      |     |            |                     |           _____________________________________________|______                                                \n",
            "   |      |     |            |                     |          |          |        |                              be_VB                                            \n",
            "   |      |     |            |                     |          |          |        |       _________________________|______                                         \n",
            "   |      |     |          is_VBZ                  |          |          |        |      |         |                   edge_NN                                    \n",
            "   |      |     |      ______|_______              |          |          |        |      |         |         _____________|________________________________        \n",
            "   |      |     |     |           tree_NN        now_RB       |          |        |      |         |        |      |      |            is_RB         dependency_NN\n",
            "   |      |     |     |              |       ______|_____     |          |        |      |         |        |      |      |        ______|______           |       \n",
            "One_PRP can_MD ._. what_WP          a_DT  and_CC        -_: the_DT linguistic_JJ -_: where_WRB   may_MD   an_DT   ,_,  )_-RRB- (_-LRB-       that_RB      a_DT    \n",
            "\n",
            "               spanning_VBG                                              \n",
            "    ________________|_______________________________                      \n",
            "   |      |         |           |               trees_NNS                \n",
            "   |      |         |           |                   |                     \n",
            "   |      |         |           |              composed_VBN              \n",
            "   |      |         |           |                   |                     \n",
            "   |      |         |           |                 of_IN                  \n",
            "   |      |         |           |                   |                     \n",
            "   |      |         |       result_NN        dependencies_NNS            \n",
            "   |      |         |           |        ___________|______________       \n",
            "are_VBP all_DT     ._.        The_DT  the_DT                  possible_JJ\n",
            "\n",
            "              solving_VBG                             \n",
            "       ____________|________________                   \n",
            "      |            |              is_VBZ              \n",
            "      |            |          ______|__________        \n",
            "      |            |         |             parsing_NN \n",
            "      |            |         |                 |       \n",
            "constraint_NN     ._.     Thus_RB        dependency_NN\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tree('is_VBZ', ['Dependency_NN', Tree('parsing_VBG', [Tree('with_IN', [Tree('rules_NNS', [Tree('LP_NNP', ['ID_NNP', '/_SYM'])])])]), 'trivial_JJ', Tree('have_VBP', ['if_IN', 'you_PRP', Tree('lexicon_NN', ['a_DT'])]), '._.']),\n",
              " Tree('uses_VBZ', ['One_PRP', Tree('approach_NN', ['a_DT', '(_-LRB-', 'declarative_JJ', ')_-RRB-', Tree('generate_VB', ['-_HYPH', 'and_CC', Tree('test_NN', ['-_HYPH'])])]), '._.']),\n",
              " Tree('are_VBP', [Tree('trees_NNS', ['Dependency_NN']), Tree('rooted_VBN', [Tree('spanning_VBG', ['trees_NNS', Tree('on_IN', [Tree('graph_NN', ['a_DT'])]), Tree('with_IN', ['n_CC', 'nodes_NNS'])])]), Tree('is_VBZ', [Tree('where_WRB', ['n_CC']), Tree('length_NN', ['the_DT', Tree('of_IN', [Tree('sentence_NN', ['the_DT', 'processed_VBN'])])])]), '._.']),\n",
              " Tree('declare_VB', ['One_PRP', 'can_MD', Tree('is_VBZ', ['what_WP', Tree('tree_NN', ['a_DT'])]), Tree('comes_VBZ', [Tree('now_RB', ['and_CC', '-_:']), Tree('part_NN', ['the_DT', 'linguistic_JJ', '-_:', Tree('be_VB', ['where_WRB', Tree('edge_NN', ['an_DT', Tree('is_RB', ['(_-LRB-', 'that_RB']), ',_,', Tree('dependency_NN', ['a_DT']), ')_-RRB-']), 'may_MD'])])]), '._.']),\n",
              " Tree('spanning_VBG', [Tree('result_NN', ['The_DT']), 'are_VBP', 'all_DT', Tree('trees_NNS', [Tree('composed_VBN', [Tree('of_IN', [Tree('dependencies_NNS', ['the_DT', 'possible_JJ'])])])]), '._.']),\n",
              " Tree('solving_VBG', [Tree('is_VBZ', ['Thus_RB', Tree('parsing_NN', ['dependency_NN'])]), 'constraint_NN', '._.'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "metadata": {
        "id": "e0lC2zLqB_lm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a18126e1-cf29-4e48-c0bd-4c5e82813c72"
      },
      "cell_type": "code",
      "source": [
        "[to_nltk_tree(sent.root) for sent in doc.sents]"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tree('is_VBZ', ['Dependency_NN', Tree('parsing_VBG', [Tree('with_IN', [Tree('rules_NNS', [Tree('LP_NNP', ['ID_NNP', '/_SYM'])])])]), 'trivial_JJ', Tree('have_VBP', ['if_IN', 'you_PRP', Tree('lexicon_NN', ['a_DT'])]), '._.']),\n",
              " Tree('uses_VBZ', ['One_PRP', Tree('approach_NN', ['a_DT', '(_-LRB-', 'declarative_JJ', ')_-RRB-', Tree('generate_VB', ['-_HYPH', 'and_CC', Tree('test_NN', ['-_HYPH'])])]), '._.']),\n",
              " Tree('are_VBP', [Tree('trees_NNS', ['Dependency_NN']), Tree('rooted_VBN', [Tree('spanning_VBG', ['trees_NNS', Tree('on_IN', [Tree('graph_NN', ['a_DT'])]), Tree('with_IN', ['n_CC', 'nodes_NNS'])])]), Tree('is_VBZ', [Tree('where_WRB', ['n_CC']), Tree('length_NN', ['the_DT', Tree('of_IN', [Tree('sentence_NN', ['the_DT', 'processed_VBN'])])])]), '._.']),\n",
              " Tree('declare_VB', ['One_PRP', 'can_MD', Tree('is_VBZ', ['what_WP', Tree('tree_NN', ['a_DT'])]), Tree('comes_VBZ', [Tree('now_RB', ['and_CC', '-_:']), Tree('part_NN', ['the_DT', 'linguistic_JJ', '-_:', Tree('be_VB', ['where_WRB', Tree('edge_NN', ['an_DT', Tree('is_RB', ['(_-LRB-', 'that_RB']), ',_,', Tree('dependency_NN', ['a_DT']), ')_-RRB-']), 'may_MD'])])]), '._.']),\n",
              " Tree('spanning_VBG', [Tree('result_NN', ['The_DT']), 'are_VBP', 'all_DT', Tree('trees_NNS', [Tree('composed_VBN', [Tree('of_IN', [Tree('dependencies_NNS', ['the_DT', 'possible_JJ'])])])]), '._.']),\n",
              " Tree('solving_VBG', [Tree('is_VBZ', ['Thus_RB', Tree('parsing_NN', ['dependency_NN'])]), 'constraint_NN', '._.'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "metadata": {
        "id": "0mO9Bv-eL_27",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "006bb3fd-c48f-4384-e62a-52261b60e1fa"
      },
      "cell_type": "code",
      "source": [
        "print(doc)"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dependency parsing with ID/LP rules is trivial if you have a lexicon. One uses a (declarative) generate-and-test approach. Dependency trees are rooted spanning trees on a graph with n nodes where n is the length of the processed sentence. One can declare what a tree is and - now comes the linguistic part - where an edge (that is, a dependency) may be. The result are all spanning trees composed of the possible dependencies. Thus dependency parsing is constraint solving.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JrDhnft9Pei6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "question1 = 'What is constraint solving'\n",
        "question2 = 'what are rooted spanning trees on a graph with n nodes'\n",
        "question3 = 'who did they surveyed'\n",
        "question4 = 'what was data'\n",
        "question5 = 'when it started'\n",
        "question6 = 'is data is theoritical'\n",
        "question_now = question5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJl3sr8FOmRv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "st = LancasterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxbsxBZkMWYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 20658
        },
        "collapsed": true,
        "outputId": "8c3e97b4-2b21-4b88-c12d-efe30bd61941"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# doc.lower()\n",
        "# doc = en_nlp(doc1)\n",
        "sentences = en_nlp(doc3.lower()).sents\n",
        "# print(sentences)\n",
        "for i,sent in enumerate(sentences):\n",
        "        roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n",
        "        print(roots)"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['academy', 'of']\n",
            "[]\n",
            "[]\n",
            "['emot', 'by', 'emot', 'discrep', 'expect', 'toegel']\n",
            "['london']\n",
            "['anand']\n",
            "['lausan']\n",
            "['help', 'help', 'with', 'respond']\n",
            "['analys', 'analys', 'interview', 'from', 'of']\n",
            "['found', 'help', 'in', 'of', 'abov', 'defin', 'defin', 'in']\n",
            "['acceiv', 'acceiv', 'control', 'for', 'of']\n",
            "['perceiv', 'as', 'with', 'auth', 'saw', 'saw', 'to', 'do', 'for', 'in']\n",
            "['point', 'to', 'of']\n",
            "['tre', 'as', 'of', 'requir', 'see', 'see', 'as', 'requir']\n",
            "['attribut', 'concern', 'help', 'to', 'attribut', 'attribut', 'by', 'attribut', 'feel', 'at', 'of']\n",
            "['contribut', 'contribut', 'of', 'concern', 'with', 'for', 'on', 'emot', 'man']\n",
            "['is', 'of', 'is', 'of', 'between', 'lab', 'be', 'in', 'of', 'control', 'bacharach']\n",
            "['parallel', 'ar', 'among', 'concern', 'of', 'to']\n",
            "['argu']\n",
            "['pect', 'of', 'as', 'und', 'gaz']\n",
            "['suggest', 'of', 'from', 'the', 'blau', 'lawl', 'is', 'of', 'is', 'of', 're-']\n",
            "['ciproc', 'ciproc', 'commit', 'perform']\n",
            "['suggest', 'with', 'enh', 'toward', 'enh', 'to', 'org', 'tsu', 'pearc', 'port']\n",
            "['show', 'to', 'gen', 'est', 'est', 'with', 'through', 'of', 'of', 'gen', 'on', 'of', 'in', 'to', 'in']\n",
            "['eisenberg']\n",
            "['et']\n",
            "['thank', 'thank', 'for', 'throughout']\n",
            "['view', 'for']\n",
            "['benefit', 'of', 'from', 'from', 'maitl', 'to', 'extend', 'extend']\n",
            "['improv', 'through', 'at', 'of', 'univers', 'of', 'of', 'academy', 'at', 'of', 'otreg', 'behavy', 'at', 'of']\n",
            "['copyright', 'of', 'of', 'reserv']\n",
            "['cop', 'to', 'without']\n",
            "['print', 'print', 'download', 'for', 'toegel', 'kilduff', 'kilduff']\n",
            "['expect', 'provid', 'provid', 'to', 'to', 'reciproc', 'doing', 'for', 'superv', 'wayn', 'shor']\n",
            "['is', 'of', 'import', 'of', 'help', 'of', 'behavy', 'between', 'man', 'remain', 'eng', 'in', 'help', 'with']\n",
            "['explain', 'of', 'overcom', 'overcom', 'avoid', 'from', 'see', 'giv', 'by', 'recipy', 'georg', 'giv', 'provid', 'provid', 'suff', 'per-', 'consequ']\n",
            "['is', 'from', 'toward', 'of', 'concern', 'in']\n",
            "['ident', 'ident', 'of', 'contribut', 'of', 'on', 'of', 'man', 'to']\n",
            "['from', 'of', 'draw', 'seek', 'contribut', 'to', 'contribut', 'on', 'research', 'research', 'to', 'tak', 'to', 'bergeron', 'bolino', 'spect', 'spect', 'dyn']\n",
            "['giv', 'on', 'giv', 'trol', 'promot', 'show', 'dis-', 'man', 'rat', 'rat', 'repres', 'dis-', 'behavy', 'cf', 'katz', 'on']\n",
            "[]\n",
            "['congru', 'to', 'of', 'of', 'see', 'reciproc', 'for', 'expect', 'expect', 'in', 'of', 'commit']\n",
            "['repres', 'of', 'not', 'of', 'study', 'is', 'is', 'is']\n",
            "['do', 'do', 'for', 'expect']\n",
            "[]\n",
            "['go', 'to', 'buy', 'buy', 'buy', 'expect', 'buy', 'buy', 'buy']\n",
            "['is', 'in', 'of', 'in']\n",
            "['they']\n",
            "[]\n",
            "['see']\n",
            "[]\n",
            "['is', 'is', 'of', 'process', 'lawl', 'between', 'man', 'man', 'with', 'in', 'diff']\n",
            "['see', 'see', 'of', 'as', 'is', 'expect', 'in', 'of', 'in', 'giv', 'giv', 'in', 'interact', 'lew', 'cat']\n",
            "['to', 'feel', 'feel', 'to', 'ar', 'to', 'provid', 'with']\n",
            "['seen', 'of', 'by', 'to', 'suff', 'promot', 'seen', 'be', 'repaid']\n",
            "['ar', 'access', 'with', 'to', 'perceiv', 'ar', 'concern', 'to', '2009']\n",
            "['set', 'disappoint', 'by', 'of', 'by', 'help']\n",
            "['tend', 'view', 'on', 'of', 'org']\n",
            "['to', 'accord', 'ident', 'ident', 'with', 'ar', 'ar', 'by', 'to', 'as', 'to']\n",
            "['eisenberg', 'eisenberg']\n",
            "[]\n",
            "['is', 'expect', 'he', 'expect', 'reciproc', 'toward', 'org']\n",
            "['puzzl', 'by', 'from', 'of', 'pay', 'pay', 'in', 'of', 'burnout', 'for', 'deal', 'with', 'emot']\n",
            "['emot']\n",
            "['includ', 'express', 'concern', 'for', 'individ', 'lar', 'consti-', 'tut', 'of', 'as', 'cit', 'behavy', 'through', 'of', 'of', 'from', 'consti-', 'in', 'develop', 'develop', 'of', 'in', 'tak', 'of', 'between', 'man', 'with', 'perceiv', 'requir', 'perceiv']\n",
            "['emot', 'in', 'in', 'involv', 'involv', 'serv', 'renew', 'to', 'of', 'to', 'encount', 'man', 'of', 'by', 'to', 'group', 'lawl', 'thy']\n",
            "['been', 'on', 'of', 'practit', 'concern', 'of', 'near', 'of', 'on', 'top']\n",
            "['bein', 'by', 'of', 'develop', 'see', 'for', 'of']\n",
            "['in', 'defin', 'reg', 'feel', 'for', 'goal']\n",
            "['97)—is', 'monit', 'at', 'by', 'ar', 'in', 'sel', 'to', 'custom']\n",
            "['expect', 'ul', 'stress']\n",
            "[]\n",
            "['dee', 'perceiv', 'with', 'in', 'in-']\n",
            "['manag-']\n",
            "['cal', 'to', 'suff', 'from']\n",
            "[]\n",
            "['to', 'requir', 'requir', 'lab', 'there-', 'ar', 'not', 'in', 'on', 'of']\n",
            "[]\n",
            "['consid', 'on', 'influ']\n",
            "[]\n",
            "['in', 'of', 'satisfy', 'challeng']\n",
            "['suggest', 'in', 'toward', 'in', 'be']\n",
            "['is', 'for', 'is', 'is', 'do', 'to', 'expery', 'in']\n",
            "['study', 'of', 'in', 'form', 'of', 'form', 'to']\n",
            "['rev', 'of', 'in', 'of', 'us', 'rev']\n",
            "['was', 'was', 'fost', 'help', 'help', 'with', 'as']\n",
            "['fect']\n",
            "['emphas', 'of', 'emphas', 'commun', 'famy', 'provid', 'lat', 'against', 'employ', 'bacharach']\n",
            "['mov', 'trad', 'in', 'as', 'toward', 'of', 'through', 'of', 'tend', 'of', 'ar', 'of', 'in', 'bacharach']\n",
            "['tend', 'in', 'becom', 'for', 'through', 'as', 'employ']\n",
            "['survey', 'survey']\n",
            "['or-', 'of', 'tak', 'in']\n",
            "['gan']\n",
            "['anticip', 'with', 'of', 'into', 'from', 'in']\n",
            "['emo-']\n",
            "['cre', 'through', 'with', 'ar', 'to', 'by', 'control', 'lawl', 'control']\n",
            "['threaten', 'from', 'in', 'threaten', 'latané', 'latané', 'across', 'affect', 'coop', 'coop', 'perform']\n",
            "['in', 'as', 'job', 'be', 'of', 'prev', 'of']\n",
            "['tak', 'tak', 'with', 'becom', 'to', 'toegel', 'kilduff', 'kilduff', 'suff', 'in', 'to']\n",
            "['from', 'interv', 'solv', 'solv', 'solv', 'eas', 'of', 'is', 'by']\n",
            "['fram', 'by', 'as', 'gift', 'of', 'rol']\n",
            "['overview', 'of']\n",
            "['us', 'us', 'with', 'of', 'investi-', 'in']\n",
            "['structure', 'foreshadow', 'the-', 'direct']\n",
            "['found', 'found', 'eng', 'in', 'eng']\n",
            "['interpret', 'interpret', 'by', 'am', 'in', 'giv', 'for', 'affect']\n",
            "['in', 'seen', 'by', 'man', 'as', 'from', 'to']\n",
            "['guid', 'guid', 'into']\n",
            "['to', 'do', 'incorp', 'in', 'of', 'toegel', 'anand']\n",
            "['aspect', 'of', 'monit', 'monit', 'of', 'not', 'look']\n",
            "['requir', 'of', 'request', 'request', 'in', 'of', 'in', 'by', 'seek']\n",
            "[]\n",
            "['ond', 'mot', 'mot', 'in', 'in', 'of']\n",
            "['concern', 'seen', 'in', 'rol', 'provid', 'to', 'provid']\n",
            "['do', 'man', 'man', 'expect', 'as', 'of', 'in', 'emot']\n",
            "['requir', 'of', 'for', 'requir', 'in', 'of', 'of']\n",
            "[]\n",
            "['bamberg', 'do', 'man', 'view', 'of']\n",
            "['prompt', 'by', 'diff', 'man', 'be', 'sid', 'on', 'of', 'prompt', 'diff', 'man', 'reciproc']\n",
            "['quest', 'prompt', 'of', 'prompt', 'from', 'on', 'of', 'man']\n",
            "['categ', 'categ', 'help', 'led', 'of', 'to']\n",
            "['address', 'in', 'build', 'build', 'toward', 'provid', 'provid', 'with', 'with', 'surfac', 'from', 'from', 'concern']\n",
            "['method', 'method']\n",
            "['study', 'study', 'of', 'provid', 'for', 'as']\n",
            "['was', 'by', 'left', 'left', 'at']\n",
            "['in', 'to', 'emphas', 'emphas', 'ef-', 'of', 'man', 'of', 'with', 'er', 'hir']\n",
            "['tripl', 'dur', 'had', 'of', 'to', 'wom']\n",
            "['as', 'in', 'receiv', 'receiv', 'was', 'in', 'loc']\n",
            "['was', 'at', 'was', 'to', 'of']\n",
            "['wer', 'est', 'with', 'of', 'achiev', 'to', 'thought', 'concern']\n",
            "['is', 'for338', 'of', 'journ', 'build', 'eisenhardt', 'eisenhardt']\n",
            "['proc', 'proc', 'across']\n",
            "['collect']\n",
            "['help', 'went', 'to', 'for', 'wer', 'to', 'in']\n",
            "['collect', 'as', 'of', 'by', 'of', 'questionnair', 'to']\n",
            "['ass', 'ass', 'be', 'provid', 'debrief', 'follow']\n",
            "['aft', 'respond', 'concern', 'construct', 'collect', 'collect', 'from', 'help', 'deal', 'with']\n",
            "['on', 'ask', 'look', 'of', 'of', 'deal', 'help', 'help', 'with', 'at', 'down', 'of', 'em-', 'plac', 'ind', 'of', 'help', 'think', 'help', 'nee', 'nee', 'in', 'of', 'with', 'problem']\n",
            "[]\n",
            "['examin', 'tend', 'with', 'of', 'as', 'us', 'us', 'categ', 'into', 'on', 'of']\n",
            "['cate-']\n",
            "['check', 'by', 'for']\n",
            "['calc', 'for', 'categ', 'calc', 'nomin', 'nomin', 'help', 'help']\n",
            "['of', 'ar', 'ad-']\n",
            "['for', 'of', 'in']\n",
            "['appear', 'as', 'on', 'of', 'of', 'appear', 'be', 'of']\n",
            "['to', 'of', 'of', 'of']\n",
            "['rang', 'from', 'rang', 'of', 'to', 'nomin', 'of']\n",
            "['see', 'for', 'of', 'exampl', 'gow', 'for']\n",
            "['study']\n",
            "['in', 'for', 'interview', 'interview', 'in', 'of', 'about', 'task', 'of', 'shadow', 'for', 'observ', 'of', 'observ', 'for', 'in', 'ing', 'across']\n",
            "['of', 'work', 'in', 'includ', 'partn', 'consult', 'man', 'interview', 'interview', 'men']\n",
            "['wer', 'for', 'involv', 'start', 'of', 'in']\n",
            "['interview', 'in', 'on', 'provid', 'pict']\n",
            "['of', 'busy', '”', 'about']\n",
            "['interview', 'interview']\n",
            "['giv', 'of', 'us', 'us', 'as', 'stop', 'collect', 'glas']\n",
            "['found', 'by', 'interview', 'interview', 'of']\n",
            "['interview', 'interview', 'men', 'wom', 'repres', 'of']\n",
            "['last', 'follow', 'in']\n",
            "['us', 'us', 'of']\n",
            "['beg', 'with', 'ask', 'tel', 'about', 'expery', 'ask', 'charact', 'employ', 'help', 'help', 'through', 'see']\n",
            "['us', 'us', 'doe', 'doe', 'vid', 'help']\n",
            "[]\n",
            "['us', 'us', 'from', 'guid', 'in']\n",
            "['uncov', 'expect', 'of', 'requir', 'in', 'produc', 'around', 'by', 'particip']\n",
            "['com', 'toegel', 'kilduff', 'toegel', 'of', 'through', 'with', 'employ']\n",
            "['provid']\n",
            "['by', 'follow', 'follow', 'employ', 'of', 'conduc', 'employ', 'provid', 'of']\n",
            "['analys', 'of']\n",
            "['show', 'of', 'emerg', 'concern', 'expect', 'reciproc']\n",
            "['us', 'us', 'softw', 'to', 'us', 'describ', 'nee', 'nee', 'emot', 'as', 'to', 'anxy', 'meet', 'target', 'dea', 'of', 'memb']\n",
            "['assembl', 'assembl', 'to', 'issu', 'into', 'of', 'see', 'in', 'with', 'label', 'in', 'is', 'in']\n",
            "['was', 'comp', 'from', 'develop', 'of', 'dimend', 'with']\n",
            "['cilit', 'analys']\n",
            "['fig']\n",
            "['perceiv', 'perceiv', 'be']\n",
            "['us', 'us', 'dis-', 'play', 'mil', 'cre', 'them', 'in']\n",
            "['help', 'in', 'of', 'build', 'build']\n",
            "['pres', 'numb', 'numb', 'provid', 'on', 'in', 'der']\n",
            "['ar', 'with', 'categ', 'e-1', 'e-1', 'mm-1', 'mm-1', 'hlm-1']\n",
            "['prefix', 'in']\n",
            "['provid', 'provid', 'concern', 'of', 'in', 'concern', 'analys', 'dimend', 'is', 'in']\n",
            "[]\n",
            "['provid', '’s', 'in']\n",
            "['”', 'emot', 'as', 'on']\n",
            "['report', 'doing', 'doing', 'by', 'of', 'abov', 'as']\n",
            "[]\n",
            "['perceiv', 'perceiv', 'emot', 'as', 'of']\n",
            "['’s', '’s', 'of', '’s', '’s', 'am']\n",
            "[]\n",
            "['emot', 'is', 'of', 'is', 'of', 'aft']\n",
            "['be']\n",
            "[]\n",
            "['provid', 'provid', 'for']\n",
            "['is', 'is', 'is', 'is', 'support']\n",
            "[]\n",
            "['view', 'emot', 'in', 'of', 'man', 'solv', 'as', 'hav', 'com', 'hav', 'hav', 'had', 'had']\n",
            "['is', 'with', 'to']\n",
            "[]\n",
            "['fig', 'of', 'emot', 'of', 'fig', 'concern', 'for', 'reciproc', 'expect', 'dimend', 'them', 'is', 'of', 'expect']\n",
            "['is', 'is', 'of', 'help', 'through', 'be']\n",
            "['’s', '’s', 'of', 'help', 'through']\n",
            "['expect', 'expect', 'commit', 'provid', 'get', 'get']\n",
            "['build', 'build', 'ask', 'ask', 'do', 'ov', 'don']\n",
            "[]\n",
            "['quot', 'from', 'for', 'provid', 'about', 'receiv']\n",
            "[]\n",
            "['expect', 'be', 'provid']\n",
            "['in']\n",
            "[]\n",
            "[]\n",
            "['quot']\n",
            "['be', 'on']\n",
            "['be', 'they', 'to', 'be', 'to']\n",
            "[]\n",
            "['expect', 'concern', 'for', 'emot']\n",
            "['see', 'see', 'reciproc']\n",
            "['support', 'support', 'want', 'return']\n",
            "['am', 'am', 'to']\n",
            "[]\n",
            "['analys', 'of', 'situ', 'in', 'of']\n",
            "['result', 'result']\n",
            "['recruit', 'recruit', 'through', 'us', 'in', 'of']\n",
            "[]\n",
            "['wer']\n",
            "['work', 'staff', 'of', 'in', 'of', 'in', 'of']\n",
            "['ing', 'than', 'in', 'in', 'of', 'was', 'includ', 'to']\n",
            "['of', 'concern', 'of', 'know', 'bil']\n",
            "['’s', 'ar', 'look']\n",
            "['hav']\n",
            "['cess']\n",
            "['is']\n",
            "['was', 'with', 'was', 'concern']\n",
            "[]\n",
            "['form', 'of', 'form', 'of', 'through', 'in', 'of']\n",
            "['interact', 'with', 'of', 'set', 'colleagu', 'cli', 'of', 'seek', 'cli']\n",
            "['said', 'becom', 'help', 'help', '”']\n",
            "['report', 'was', 'nee', 'read', 'intelli-', 'interview']\n",
            "['was', 'expect', 'pres', 'with']\n",
            "['reinforc', 'reinforc', 'in', 'reinforc', '“', 'exchang', 'with', 'los', 'of', 'in', 'to']\n",
            "['on', 'of']\n",
            "['observ', 'toegel', 'kilduff', 'groom', 'of', 'with', 'got', 'put', 'on']\n",
            "['describ', 'describ', 'in', 'of']\n",
            "['regard', 'regard', 'bring', 'to', 'bring', 'meas', 'control']\n",
            "['track', 'candid', 'with', 'achiev', 'spit', 'spit']\n",
            "['goe', 'everyth', 'they', 'through']\n",
            "[]\n",
            "['had', 'had', 'had', 'help', 'with']\n",
            "['dur', 'of', 'ask', 'be', 'of', 'emot', 'in']\n",
            "['disagree', 'of']\n",
            "['of', 'talk', 'they', 'to', '’s', 'to']\n",
            "['monit']\n",
            "['of', 'on', 'meas', 'is', 'for']\n",
            "['ar', 'grad']\n",
            "['’m', 'tak']\n",
            "['hav', 'hav', 'in', 'term', 'is']\n",
            "['is', 'exam', 'is', 'know', 'talk', 'talk', 'about']\n",
            "['ad', 'ar', 'of']\n",
            "['ar', 'hav', 'hav', 'with', 'hav', 'hav', 'with', 'direct']\n",
            "['focus', 'on', 'on']\n",
            "['describ', 'us', 'with', '“', 'in', 'run', 'run']\n",
            "[]\n",
            "['know', 'plac', 'plac', 'of', 'interview']\n",
            "['do']\n",
            "['is', 'of', 'interview', 'of']\n",
            "['’s', 'about']\n",
            "['been']\n",
            "['ov']\n",
            "['go', 'to', 'go', 'ar', 'ar']\n",
            "['believ', 'in', 'run', 'run']\n",
            "['start', 'tel', 'do', 'do', 'do']\n",
            "['hav', 'crush']\n",
            "['win', 'win', 'heart']\n",
            "['ceo']\n",
            "['com', 'on', 'man', 'in']\n",
            "[]\n",
            "['hav', 'hav', 'got', 'work', 'in', 'lik', 'in', 'is']\n",
            "['work']\n",
            "['hurt']\n",
            "[]\n",
            "[]\n",
            "['report', 'of', 'report', 'help', 'help', 'with', 'of', 'encompass', 'of']\n",
            "['report', 'of', 'expect', 'provid', 'provid', 'with']\n",
            "['giv', 'of', 'nee', 'nee', 'recount', 'prompt', 'by', 'of', 'prompt', 'with', 'recount', 'prompt', 'by', 'through']\n",
            "['had', 'had', 'of']\n",
            "['mm-']\n",
            "[]\n",
            "['ment', 'anxiet-', 'concern', 'meet']\n",
            "['get', 'of']\n",
            "['report', 'was', 'emot', 'on', 'at', 'of', 'of', 'janu', 'tend']\n",
            "['rel', 'of', 'to']\n",
            "[]\n",
            "['ment', 'ment', 'as', 'il', 'of', 'ment']\n",
            "[]\n",
            "['split', 'with', 'was']\n",
            "['fal', 'was']\n",
            "['cont']\n",
            "['stressed', 'at', 'as', 'of', 'problem', 'outsid']\n",
            "[]\n",
            "['said', 'deal', 'with']\n",
            "[]\n",
            "['in', 'tion', 'driv', 'driv', 'seek']\n",
            "['quest', 'quest', 'across']\n",
            "['incorp', 'incorp', 'help', 'of']\n",
            "['show', 'in', 'incorp', 'incorp', 'incorp', 'of', 'in', 'of', 'by', 'ext']\n",
            "['level', 'level', 'level']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['ind', 'ind', 'receiv', 'from', 'of', 'ind', 'within', 'ind']\n",
            "['adjust', 'tak', 'of', 'of', 'in', 'unadjust', 'in']\n",
            "['diff', 'diff', 'at']\n",
            "['tion', 'into', 'of', 'in']\n",
            "['receiv', 'receiv', 'as', 'emot', 'receiv', 'receiv']\n",
            "['valu', 'in', 'valu', 'for', 'valu', 'nomin', 'nomin', 'help', 'allevy']\n",
            "['tress']\n",
            "['ar', 'ar', 'for', 'of']\n",
            "['show', 'in', 'by', 'of', 'by', 'in', 'had', 'had', 'by', 'from', 'by', 'of']\n",
            "['wer', 'of', 'of', 'in', 'of', 'for', 'with', 'was', 'of']\n",
            "['of']\n",
            "['nomin', 'at', 'of', 'nomin', 'at', 'of', 'emot', 'wer', 'in', 'from', 'to']\n",
            "['was', 'with', 'to']\n",
            "[]\n",
            "[]\n",
            "['explain', 'tend', 'expect', 'on', 'in', 'provid', 'expect', 'in', 'of']\n",
            "['man-']\n",
            "['stat', 'hop', 'said']\n",
            "['to', 'help', 'help']\n",
            "['of', 'hav', 'hav', 'with']\n",
            "['ex-']\n",
            "['talk', 'about', 'with']\n",
            "['work', 'for', 'develop', 'develop']\n",
            "['”']\n",
            "['between']\n",
            "[]\n",
            "['was', 'becaus']\n",
            "['reflect', 'in', 'of', 'stat', 'think', 'expect', 'expect', 'provid', 'to']\n",
            "['provid']\n",
            "['in', 'in', 'to', 'found', 'report', 'of', 'employ', 'of', 'emot', 'in']\n",
            "['cern', 'of', 'involv', 'proff', 'with', 'wait', 'for', 'request']\n",
            "['structure', 'concern', 'summ', 'in', 'summ', 'process']\n",
            "['solicit', 'across', 'of', 'lin', 'mot', 'solicit']\n",
            "['suggest', 'solicit']\n",
            "['ploy', 'from', 'appear']\n",
            "['stat', 'is', 'for']\n",
            "['”']\n",
            "['suggest', 'emphas', 'tri', 'mot', 'feel', 'wer', 'in', 'with', 'approach', 'approach', 'for', 'feel', 'feel', 'real']\n",
            "['ar']\n",
            "['wer', 'affect', 'affect', 'at']\n",
            "['show', 'of', 'overcom', 'wer', 'of', 'solicit', 'with']\n",
            "['summ', 'summ']\n",
            "['scan', 'for', 'of', 'by', 'oft']\n",
            "['see', 'is', 'approach']\n",
            "['’s', 'feel']\n",
            "['to']\n",
            "['say', 'is']\n",
            "['say', 'emot']\n",
            "['mot']\n",
            "[]\n",
            "['mot', 'of']\n",
            "['proact']\n",
            "['help', 'scan', 'for', 'in', 'in']\n",
            "['reward', 'reward', 'impress']\n",
            "['off', 'off', 'want', 'want']\n",
            "['lik']\n",
            "[]\n",
            "['introvert', 'lik', 'keep', 'to']\n",
            "['mm-3']\n",
            "['explain', 'go', 'to', 'see', 'ar']\n",
            "['say', 'talk', 'for', 'of', 'hav', 'hav', 'of', 'try', 'draw']\n",
            "[]\n",
            "['was', 'by', 'stat', 'pref', 'car', 'for', 'ar', 'ar', 'going', 'ar', 'ar']\n",
            "['seem', 'wer', 'in', 'tun', 'to', 'among', 'in', 'spot', 'spot']\n",
            "[]\n",
            "['describ', 'of']\n",
            "['push', 'push', 'about', 'ar']\n",
            "['ar', 'ask', 'stressed', 'am', 'to']\n",
            "['help', 'tak']\n",
            "['disclos', 'disclos', 'respond', 'man', 'disclos', 'discrep']\n",
            "['see', 'see', 'as', 'help', 'in', 'of', 'in']\n",
            "['see', 'in', 'as']\n",
            "['man', 'man']\n",
            "['refus', 'help', 'with']\n",
            "['concern', 'in', 'in', 'concern', 'of', 'provid']\n",
            "['emerg', 'of', 'act', 'as', 'emot', 'from']\n",
            "['was', 'among', 'of', 'view', 'of', 'in']\n",
            "['thought', 'accru', 'cop', 'with', 'affect', 'affect', 'money', 'sal', 'provid', 'provid', 'get', 'get', 'on']\n",
            "['of', 'to', '’s']\n",
            "['want', 'leav', 'want', 'be', 'at', 'hav', 'on']\n",
            "['lik', 'hav']\n",
            "['want', 'be', 'around', 'be', 'of', 'ar', 'rub', 'on']\n",
            "['mm-5']\n",
            "['com', 'hav', 'provid', 'spread', 'it', 'lik']\n",
            "['tak', 'tak', 'of', 'expect', 'of', 'with', 'commit', 'in', 'summ', 'summ']\n",
            "['mo-']\n",
            "['charact', 'for', 'for', 'charact', 'express', 'express', 'about']\n",
            "['tend', 'mak', 'about', 'for', 'provid', 'is', 'of', '’s', '’s', 'am']\n",
            "[]\n",
            "['mm-5']\n",
            "['com-']\n",
            "['cam', 'help', 'through']\n",
            "[]\n",
            "['lik', 'going']\n",
            "['mad', 'clear', 'mak', 'interest', 'hav', 'hav', 'in', 'peopl', 'of', 'stim']\n",
            "['stat', 'hav', 'mak', 'tick', 'help', 'hap', 'in', 'work']\n",
            "['tend', 'in', 'provid', 'to', 'was', 'in', 'of']\n",
            "['was', 'remark', 'pract', 'help', 'with']\n",
            "[]\n",
            "['spok', 'about', 'in', 'with', '’s', '’s']\n",
            "[]\n",
            "['hlm-1']\n",
            "['to', 'ment', 'by', 'with', 'solicit', 'solicit', 'in']\n",
            "['nurt', 'express', 'express', 'nurt', 'gain', 'gain', 'gain', 'from', 'of', 'of', 'of', 'from']\n",
            "['not']\n",
            "['help', 'help', 'en', 'en', 'on', 'start', 'achiev', 'delight', 'delight', 'is', 'is', 'in', 'know', 'help', 'help']\n",
            "['said', 'said', 'to', 'about', 'help', 'help']\n",
            "['say', 'he', 'say', 'to', 'thank', 'to']\n",
            "['is', 'is', 'recognit', 'get', 'from']\n",
            "['mm-2']\n",
            "['show', 'in', 'show', 'with', 'view', 'view']\n",
            "['tend', 'wer', 'und', 'reciproc']\n",
            "['tak', 'between']\n",
            "['quest', 'quest', 'fig', 'und']\n",
            "['tak']\n",
            "[]\n",
            "['in', 'of', 'in', 'refus', 'refus', 'stop']\n",
            "['in', 'off', 'off', 'want', 'want']\n",
            "['in', 'provid', 'provid', 'by']\n",
            "['consid', 'consid', 'be', 'of']\n",
            "['is', 'answ', 'of', 'regard', 'regard', 'of', 'show', 'in']\n",
            "['with', 'by', 'be', 'of', 'as']\n",
            "['as', 'from', 'from']\n",
            "['as']\n",
            "['concern', 'provid', 'on']\n",
            "['think', '’m', '’m']\n",
            "['feel', 'in', 'to', 'about', '”']\n",
            "['in', 'of', 'man', 'doing', 'doing', 'by']\n",
            "['is', 'abov', 'as', 'man']\n",
            "['perceiv', 'perceiv', 'be']\n",
            "['diff', 'of']\n",
            "['show', 'in', 'tend', 'see', 'provid', 'as', 'of', 'of', 'in', 'of']\n",
            "['employ', 'toegel', 'kilduff', 'kilduff', 'emot', 'in', 'on', 'of']\n",
            "['for', 'with', 'in', 'from', 'of', 'ind', 'of', 'had', 'had', 'at']\n",
            "['feel']\n",
            "['bottl', 'bottl']\n",
            "['in', 'look', 'for', 'tend', 'be']\n",
            "[]\n",
            "['consid', 'consid']\n",
            "['is', 'in', 'is', 'is', 'in', 'ask', 'ar', 'cop']\n",
            "['’s', '’s']\n",
            "['nee', 'ar']\n",
            "['com', 'with']\n",
            "['reinforc', 'in', 'on', 'of', 'by', 'of']\n",
            "['ask', 'sought', 'in', 'for', 'cre', 'was', 'was', 'of']\n",
            "['expect', 'help', 'cop', 'with']\n",
            "['remark', 'being', 'set']\n",
            "['’s', 'achiev', 'achiev']\n",
            "['do', 'provid']\n",
            "['of', 'suggest', 'expect', 'is', 'of', 'aft', 'provid', 'be', 'do']\n",
            "['of', 'between', 'hav', 'hav', 'at', 'bring', 'bring']\n",
            "['’s']\n",
            "['job', 'support', 'ar', 'is', 'is']\n",
            "['help', 'of', 'requir']\n",
            "['to', 'in', 'diff', 'felt', 'repay', 'for', 'receiv', 'fig', 'of', 'in']\n",
            "['just', 'was', 'by', 'repaid', 'help', 'help', 'with', 'by']\n",
            "['for']\n",
            "['argu']\n",
            "[]\n",
            "['get', 'as', 'of', 'of']\n",
            "['benefit', 'is']\n",
            "[]\n",
            "['get', 'get', 'from']\n",
            "['help', 'they', 'help']\n",
            "['“', 'provid', 'than']\n",
            "['port']\n",
            "[]\n",
            "['of', 'is', 'in', 'help', 'sal', 'increas', 'remain', 'is', 'is', 'of', 'is']\n",
            "['gav', 'of', 'with', 'in', 'gav']\n",
            "[]\n",
            "['think', 'is', 'he', 'is']\n",
            "['get', 'get', 'doing', 'doing']\n",
            "['emot', 'from']\n",
            "['gav', 'gav', 'gav', 'doing', 'doing', 'help']\n",
            "['was', 'as', 'of', 'help', 'help', 'easy']\n",
            "['is', 'is', 'of', '’s', '’s']\n",
            "['say', 'of', 'about', 'outsid', 'about']\n",
            "['ut', 'on', 'man', 'ut']\n",
            "['’s', 'tackl']\n",
            "['benefit', 'from', 'be', 'affect']\n",
            "['get', 'get', 'achiev', 'achiev']\n",
            "[]\n",
            "['hlm-1']\n",
            "['profess', 'profess', 'reciproc', 'of', 'in']\n",
            "['not', 'am', 'am', 'to']\n",
            "['interview', 'of', 'ment', 'expect', 'of']\n",
            "['to', 'approach', 'approach', 'for', 'tend', 'attribut', 'to']\n",
            "['quest', 'quest', 'of', 'pattern', 'from', 'provid']\n",
            "['exhibit', 'exhibit', 'help']\n",
            "['class', 'from', 'help', 'bas', 'of', 'provid', 'eng', 'in', 'repres', 'of', 'valid', 'valid', 'of', 'help', 'of', 'list', 'list', 'of', 'help', 'help', 'help']\n",
            "[]\n",
            "['refram']\n",
            "['transform']\n",
            "[]\n",
            "[]\n",
            "[]\n",
            "['%', '%', '%', 'ind', 'provid', 'to', 'in']\n",
            "[]\n",
            "['report', 'provid', 'of', 'provid', 'to']\n",
            "['perc', 'of', 'to', 'of']\n",
            "['%', 'transform', 'adv']\n",
            "['tak', 'of', 'through', 'refer', 'refer', 'provid', 'of']\n",
            "['wer', 'concern', 'assocy', 'with']\n",
            "['with', 'to', 'of', 'rev']\n",
            "['tiv', 'trac', 'to', 'issu']\n",
            "['provid', 'emot', 'provid', 'by']\n",
            "['said', 'about', 'sit', 'to', 'list']\n",
            "['describ', 'describ', 'help', '“', 'get', 'off', 'problem']\n",
            "['wer', 'role-', 'model', 'emot', 'by', 'op', 'op', 'op', 'op', 'let', 'of', 'talk']\n",
            "['ind', 'help', 'of', 'help', 'ing', 'by']\n",
            "['describ', 'by', 'describ']\n",
            "[]\n",
            "['involv', 'of', 'involv', 'saw', 'saw']\n",
            "['suggest', 'from', 'gav', 'gav', 'gav', 'cop', 'cop']\n",
            "['in', 'of', 'tabl', 'by', 'enco', 'refram', 'through', 're-', 'flect', 'of', 'in']\n",
            "['describ', 'by', 'in', 'help', 'from', 'see', 'of', 'story', 'see', 'pict', 'describ', 'think-', 'from', 'perspect', 'was', '“', 'at', 'from', 'view', 'put', 'in', 'see', 'from']\n",
            "[]\n",
            "['is', 'chang', 'into']\n",
            "['in', 'mad', 'describ', 'ind', 'was', 'of']\n",
            "['of', 'to', 'ment', 'ment']\n",
            "['not']\n",
            "[]\n",
            "['laugh']\n",
            "['feel', 'was', 'sel', 'in']\n",
            "[]\n",
            "['e-11']\n",
            "['eng', 'of', 'in', 'provid', 'toegel', 'kilduff', 'from']\n",
            "['describ', 'describ', 'say', 'do', 'do', 'is']\n",
            "['emot', 'of']\n",
            "['not', 'view', 'view', 'ing', 'across', 'promot', 'perform', 'promot']\n",
            "['choos', 'foc', 'been', 'in']\n",
            "['with', 'focus', 'on', 'of', 'between', 'man', 'with', 'to', 'rol']\n",
            "['emerg', 'of', 'from', 'in']\n",
            "['lis-']\n",
            "['ten', 'provid']\n",
            "['by', 'as', 'of', 'of', 'was', 'of', 'cern', 'of', 'for']\n",
            "['follow', 'of', 'follow', 'rout']\n",
            "['was', 'of', 'prompt', 'of', 'on', 'receiv']\n",
            "['show', 'wer', 'outcom', 'in', 'by']\n",
            "['fig', 'fig', 'summ', 'of', 'by', 'to', 'for', 'outcom', 'by', 'of', 'to']\n",
            "['rel', 'on', 'of', 'provid', 'provid', 'to', 'of', '“', 'with', 'expery']\n",
            "['describ', '“', 'ar', 'about', 'in']\n",
            "['said', 'about', 'look', 'to', 'see', 'as', 'com', 'fig', 'rel', 'of', 'to', 'of', 'provid']\n",
            "['outcom', 'of']\n",
            "['shortcut']\n",
            "[]\n",
            "['list', 'elab']\n",
            "[]\n",
            "['refram', 'through']\n",
            "['adv', 'man', 'of', 'lead', 'qual']\n",
            "['emot', 'emot', 'without', 'indebt', 'sid', 'man', 'perceiv', 'of', 'breach', 'breach', 'of', 'seen', 'off', 'as', 'of', 'rol', 'rol', 'of', 'of', 'in', 'provid', 'provid', 'man', 'stress', 'burnout', 'in', 'of', 'stress', 'cre', 'depend', 'subordin', 'of', 'about', 'to', 'with', 'of', 'worry', 'with']\n",
            "['is', 'is']\n",
            "['dis-', 'of']\n",
            "['was', 'for', 'help', 'help', 'by', 'in', 'tend', 'in', 'attribut', 'as', 'as']\n",
            "['for', 'grant', 'wisdom', 'to', 'reliev', 'from', 'repay', 'in']\n",
            "['ceiv', 're-', 'ceiv', 'help', 'regard', 'becaus', 'receiv', 'from']\n",
            "['for', 'argu']\n",
            "['tak', 'tak', 'step', 'into', 'is', 'be']\n",
            "['’s', '’s']\n",
            "['feel', 'ar', 'ar', 'of', 'get']\n",
            "['build', 'build', '’s', '’s']\n",
            "[]\n",
            "['e-14']\n",
            "[]\n",
            "['work', 'in', 'of', 'ar', '”']\n",
            "['was', 'to', 'emot', 'with', 'pattern', 'for', 'provid', 'wer', 'ex-']\n",
            "['of', 'fed', 'fed', 'to', 'on', 'of', 'for', 'think', 'mak', 'mak', 'thank', 'for', 'as']\n",
            "['wer', 'man', 'caus', 'expect', 'caus']\n",
            "['vid', 'pro-', 'tend', 'cut', 'be', 'perceiv', 'be', 'of']\n",
            "['for', 'provid', 'provid', 'was', 'at', 'of']\n",
            "['express', 'express', 'is', 'is']\n",
            "['know', 'goe', 'get', 'get', 'for']\n",
            "['mm-']\n",
            "['was', 'process', 'of', 'through', 'of', 'stressed', 'tak', 'with']\n",
            "['as', 'of']\n",
            "['rely', 'provid', 'be', 'be', 'tak', 'on']\n",
            "['illust', 'by', 'illust', 'fail']\n",
            "['was', 'of', 'going', 'memb', 'outsid', 'spent', 'spent', 'of', 'with', 'get', 'through', 'at', 'giv']\n",
            "['was', 'ing', 'said', 'leav', 'leav']\n",
            "['of', 'to']\n",
            "['admit', 'was']\n",
            "['feel']\n",
            "['put', 'put', 'of', 'into', 'as', 'on']\n",
            "['of', 'said', 'seem', 'for']\n",
            "[]\n",
            "['said', 'am', 'for', 'feel', 'resign']\n",
            "[]\n",
            "['was', 'real']\n",
            "['think']\n",
            "['thought', '’s', '’s']\n",
            "[]\n",
            "['real', 'is', 'is', 'of']\n",
            "['tak']\n",
            "['think', 'is', 'is', 'of']\n",
            "['summ', 'with', 'of', 'in', 'ment', 'ment', 'for', 'in', 'of', 'by', 'as', 'of']\n",
            "['jok', 'becom', 'becom', 'aunt', 'for', 'of']\n",
            "['wer', 'to', 'concern', 'dam', 'in', 'to', 'of']\n",
            "['seen', 'by', 'saw', 'saw']\n",
            "['leagu', 'beyond', 'doing', 'doing']\n",
            "['view', 'in', 'view', 'to', 'of']\n",
            "['of', 'for', 'was', 'approach', 'explain', 'of', 'kind', 'want', 'think', 'am']\n",
            "['giv', 'bring', 'to', 'of', 'by', 'with', 'stood', 'think', '’s', 'for']\n",
            "['ple', 'label', 'by', 'as']\n",
            "['concern', 'lead', 'to', 'of', 'be-', 'caus', 'of']\n",
            "['“', 'toegel', 'kilduff']\n",
            "['know', 'thing', 'be', 'for', 'benefit']\n",
            "[]\n",
            "['dent', 'trust']\n",
            "['manag-']\n",
            "['er', 'be', 'of', 'portray']\n",
            "['wer', 'of', 'wer', 'und', 'from', 'track', 'of', 'in', 'of', 'of', 'to']\n",
            "['tend', 'as', 'provid', 'to']\n",
            "['wer', 'monit', 'of', 'off']\n",
            "['describ']\n",
            "['sion', 'in', 'of', 'of', 'as', 'tend']\n",
            "['expect', 'recipro-', 'with', 'commit']\n",
            "['resist', 'on', 'ordin', 'in', 'resist', 'of']\n",
            "['as', 'contrib-']\n",
            "['at', 'of', 'tend', 'attribut', 'attribut', 'by', 'assum', 'ar', 'ar']\n",
            "['help', 'reliev', 'of', 'of', 'for', 'cre', 'in']\n",
            "[]\n",
            "['elab', 'elab']\n",
            "['on', 'of', 'ar', 'leav', 'at', 'of', 'in', 'provid', 'provid', 'from', 'er', 'in', 'of']\n",
            "['worry', 'in', 'about', 'to', 'with', 'receiv', 'emot', 'abov']\n",
            "['develop', 'develop', 'on', 'provid', 'provid', 'of', 'requir']\n",
            "['discuss', 'from', 'crit', 'the', 'in', 'forg', 'of', 'of']\n",
            "['discrep', 'of', 'of', 'discrep']\n",
            "['to', 'focus', 'on', 'of', 'behavy', 'bowl', 'bowl', 'settoon', 'settoon', 'spitzmul', 'dyn', 'with', 'to', 'provid']\n",
            "['achiev', 'of', 'achiev', 're-', 'alloc', 'podsakoff', 'whit', 'podsakoff', 'suff', 'overload', 'stress', 'conflict', 'bolino', 'stress', 'suff', 'frost']\n",
            "['neglect', 'neglect', 'of', 'provid', 'receiv', 'know', 'diff', 'superv', 'consid', 'on', 'of', 'subordin']\n",
            "['striking', 'in']\n",
            "[]\n",
            "['acceiv', 'employ', 'acceiv', 'control', 'for', 'of', 'in', 'with', 'on', 'compass', 'rel', 'cameron', 'dutton']\n",
            "['in', 'monit', 'was', 'not', 'look']\n",
            "['seem', 'in', 'in', 'permit', 'explic-', 'for']\n",
            "['discrep', 'between', 'employ', 'on', 'consid', 'of']\n",
            "['ind', 'attribut', 'attribut', 'as', 'expect', 'as', 'of', 'by']\n",
            "['report', 'perceiv', 'as', 'respond', 'to', 'as', 'of', 'going']\n",
            "['particu-', 'in', 'of', 'inter-']\n",
            "['is', 'off', 'off', 'cit', 'as', 'of', 'as', 'anyth', 'ven', 'dyn', 'cum', 'mclean', 'dyn', 'of']\n",
            "['ar', 'provid', 'to', 'ar', 'emot', 'as', 'anyth']\n",
            "['help', 'perceiv', 'as', 'with', 'as', 'with', 'of', 'saw', 'saw', 'to', 'do', 'for', 'in']\n",
            "['rel']\n",
            "['suf-']\n",
            "['fer', 'fail', 'reciproc', 'cotterel']\n",
            "['in', 'of', 'found', 'found']\n",
            "['ex-']\n",
            "['pect', 'in', 'of', 'commit', 'commit', 'fail', 'recogn', 'in', 'went', 'to', 'help', 'man', 'of', 'respond', 'am', 'am']\n",
            "[]\n",
            "['fail', 'acknowledg', 'reciproc', 'by']\n",
            "['tend', 'interpret', 'saw', 'of', 'as', 'of']\n",
            "['suggest', 'gen', 'am', 'gen', 'gen', 'on', 'of', 'repay', 'repay', 'with']\n",
            "['expect', 'in']\n",
            "['ploy', 'see', 'as', 'expect', 'em-', 'show']\n",
            "['ar', 'on']\n",
            "['managers’', 'managers’', 'to', 'of']\n",
            "['with', 'to']\n",
            "[]\n",
            "['result', 'of', 'in', 'of', 'as', 'anxy', 'insec', 'helpless']\n",
            "['from', 'from', 'ploy', 'is', 'of', 'than', 'by', 'of', 'enco', 'shown']\n",
            "['to']\n",
            "[]\n",
            "['suggest']\n",
            "['view', 'concern', 'is', 'is', 'required—', 'view', 'fuel', 'in']\n",
            "['for']\n",
            "['clud', 'clud', 'disappoint', 'for', 'includ', 'in']\n",
            "['to', 'ar', 'help', 'for', 'it', 'grow']\n",
            "['man']\n",
            "['pear', 'of', 'from', 'tend', 'with', 'increas', 'commit', 'commit', 'armel', 'eisenberg', 'fasolo']\n",
            "['en-', 'as', 'for', 'ar']\n",
            "['see', 'qual', 'satisfact', 'of', 'see', 'in', 'bacharach', 'bamberg']\n",
            "['behav-', 'gen', 'gen']\n",
            "['fail']\n",
            "['to', 'the', 'recogn', 'in', 'bed', 'neglect', 'neglect', 'view', 'lead', 'view']\n",
            "['is', 'of', 'emphas', 'of', 'concern', 'in']\n",
            "['from', 'includ', 'includ', 'to', 'nee', 'avolio']\n",
            "[]\n",
            "['help', 'deal', 'with', 'stress', 'in', 'workplac', 'rafferty']\n",
            "['in']\n",
            "['ar', 'ar', 'of', 'attribut', 'with', 'in', 'of', 'lead']\n",
            "['see', 'see', 'in', 'help', 'as', 'someth']\n",
            "['deny']\n",
            "['from', 'seen', 'as', 'going', 'beyond', 'of', 'toegel', 'kilduff', 'to', 'see', 'as', 'ar']\n",
            "['spond', 'with', 'to', 'enh']\n",
            "['to', 'tend', 'provid', 'provid', 'with', 'by', 'tend', 'low']\n",
            "['ar', 'of', 'is', 'incit', 'ar']\n",
            "['help', 'redress', 'of', 'explain', 'between', 'the', 'stat', 'avolio', 'walumbw']\n",
            "['link', 'in', 'individ']\n",
            "[]\n",
            "['hav', 'in', 'hav', 'on', 'on', 'exit', 'behavy']\n",
            "[]\n",
            "['lead', 'lead', 'with', 'for']\n",
            "['in-', 'describ', 'influ', 'of', 'as', 'of']\n",
            "['contribut', 'to']\n",
            "['control', 'ov', 'emo-', 'tion']\n",
            "['emphas', 'in', 'firm', 'ment', 'control', 'behavy']\n",
            "['nism', 'control', 'in', 'to']\n",
            "['confront', 'carry', 'am-', 'giv', 'monit', 'monit', 'fost', 'atmosph']\n",
            "['tiv', 'control', 'to', 'crush', 'man', 'work', 'has', 'for', 'for', 'expl', 'in']\n",
            "['investig', 'in', 'is', 'is', 'of', 'be', 'ov', 'behavy', 'on']\n",
            "['giv', 'ar', 'see', 'as']\n",
            "['contribut', 'to', 'of', 'emot', 'in', 'see', 'for']\n",
            "['repres', 'in', 'repres', 'of', 'of', 'includ', 'of']\n",
            "['tion']\n",
            "['lik', 'org', 'investig', 'feat', 'of', 'ov']\n",
            "['tend', 'on', 'for', 'of', 'as', 'neces-', 'in', 'themselv']\n",
            "['rev', 'rev', 'of', 'sign', 'giv', 'cut']\n",
            "['was', 'to', 'wer', 'in', 'form', 'form', 'of', 'in', 'of', 'concern']\n",
            "['is', 'act', 'in', 'interact', 'with', 'in', 'provid', 'of', 'of', 'to', 'ask', 'for', 'bacharach']\n",
            "['liv', 'of', 'to', 'of', 'in', 'of', 'rais']\n",
            "['cern', 'of', 'to']\n",
            "['in', 'ar', 'pow', 'ar', 'of']\n",
            "['develop', 'to', 'try', 'in', 'with', 'of', 'as', 'ploy', 'mor', 'roethlisberg', 'provid', 'for', 'childr', 'baron', 'dobbin', 'elimin', 'od']\n",
            "['investig', 'investig', 'of', 'on', 'of', 'in', 'rest', 'as', 'prev', 'of', 'in', 'ar', 'of']\n",
            "['surfac', 'surfac', 'of', 'on', 'to', 'match', 'of', 'ident', 'from', 'expery']\n",
            "['list', 'in', 'recov', 'from', 'emot']\n",
            "['is', 'provid', 'provid', 'of', 'help', 'summ', 'in']\n",
            "['mov', 'beyond']\n",
            "['striking', 'in', 'is', 'of', 'giv']\n",
            "['2007)—are', 'mak', 'recov', 'recov', 'valid', 'support', 'refram', 'of', 'recr', 'concret', 'help', 'help', 'in', 'on', 'of']\n",
            "['investig', 'result', 'of', 'to', 'in', 'for', 'employ', 'than', 'interv']\n",
            "['improv', 'improv', 'bind', 'to', 'cre', 'be', 'martin', 'bind', 'ploy', 'for', 'receiv']\n",
            "['investig', 'suggest', '“', 'be', 'be', 'retain']\n",
            "['in', 'of', 'with', 'from', 'feel', 'rat', 'as', 'of']\n",
            "['work', 'investig', 'plan', 'for', 'constitut', 'expos', 'to']\n",
            "[]\n",
            "['find', 'work', 'in', 'as', 'process', 'at', 'pos', 'to']\n",
            "['en', 'en', 'seek', 'from', 'intrud', 'from', 'frost']\n",
            "['know', 'tend', 'work', 'in', 'to', 'from', 'satisfact', 'satisfact', 'tiv', 'oldham', 'creas', 'see', 'in', 'review']\n",
            "['expl', 'en', 'of', 'wal', 'en', 'among', 'facilit', 'of', 'to', 'by', 'concern', 'for']\n",
            "['to', 'list', 'solv', 'seem', 'play', 'of', 'therap', 'goldstein', 'about', 'list', 'to', 'of']\n",
            "['seem', 'to', 'to', 'includ', 'tak', 'of', 'of']\n",
            "['investig', 'rel', 'between', 'of', 'expect', 'concern', 'known', 'in', 'rel', 'displac', 'onto', 'thought', 'feel-', 'ing', 'toward', 'of']\n",
            "['becaus', 'of', 'diff', 'for', 'is', 'of', 'between', 'practit', 'is', 'in', 'therapy', 'william']\n",
            "['to', 'act', 'as', 'tak', 'tak', 'as']\n",
            "['rel', 'rel']\n",
            "['ar', 'with', 'to', 'of', 'to', 'deem']\n",
            "['exploit', 'for', 'exploit', 'of', 'giv', 'giv', 'toegel', 'kilduff', 'kilduff', 'giv', 'of', 'as', 'superv']\n",
            "['expl', 'affect', 'between', 'men', 'with', 'to', 'expery', 'simon', 'expery', 'express', 'affect', 'in']\n",
            "['lik', 'off', 'ar', 'of', 'off', 'to']\n",
            "['demonst', 'demonst', 'between', 'men', 'view', 'by', 'for', 'express', 'express', 'ang']\n",
            "[]\n",
            "['on', 'ask', 'acceiv', 'from', 'in', 'to', 'los']\n",
            "['imply']\n",
            "['protect', 'protect', 'from', 'on', 'of', 'that']\n",
            "[]\n",
            "['ow', 'help', 'help', 'with']\n",
            "['nity', 'help', 'with', 'is', 'in', 'giv']\n",
            "['provid', 'request', 'contribut', 'of']\n",
            "['bal', 'on', 'of', 'man-', 'of', 'of', 'by', 'of', 'in', 'on', 'for']\n",
            "['follow', 'of', 'defin', 'defin', 'as', 'emot', 'in', 'requir', 'requir', 'by']\n",
            "['to', 'interact', 'with', 'in']\n",
            "['vest', 'of', 'to', 'to', 'pow', 'in', 'just', 'attribut', 'to', 'of', 'to', 'of', 'person', 'overcom', 'to']\n",
            "['acknowledg', 'beyond', 'interpret', 'interpret', 'help']\n",
            "['adv', 'step', 'beyond', 'of']\n",
            "['giv', 'from', 'organiza-']\n",
            "['educ', 'concern', 'of', 'help', 'with', 'by', 'as', 'part', 'of', 'within', 'of']\n",
            "['clim']\n",
            "['cf']\n",
            "['eisenberg', 'eisenberg']\n",
            "[]\n",
            "['be', 'for']\n",
            "['ar', 'ascrib', 'on', 'of', 'to', 'qual']\n",
            "['at', 'educ', 'in', 'of', 'com', 'with']\n",
            "['is', 'not', 'to', 'in']\n",
            "['ind', 'handl', 'of', 'hav', 'as', 'frust', 'ill-', 'fail', 'ship', 'ship', 'kahn']\n",
            "['of', 'is', 'process', 'of', 'stressed', 'tak', 'with']\n",
            "['as']\n",
            "[]\n",
            "['nee', 'nee', 'on', 'for', 'nee']\n",
            "['ar', 'in', 'help', 'with', 'ar', 'as', 'on', 'hand']\n",
            "['investig', 'valid', 'on', 'of', 'in', 'in']\n",
            "['conclud']\n",
            "['empow', 'of', 'to', 'in', 'of']\n",
            "['involv', 'involv', 'assess', 'reward', 'plin', 'employ']\n",
            "['hav', 'hav', 'in', 'of']\n",
            "['in', 'encompass', 'in', 'of', 'of', 'involv', 'of', 'in', 'of']\n",
            "['control', 'of', 'in', 'monit', 'seem']\n",
            "['in', 'focus', 'focus', 'am']\n",
            "['to', 'repres', 'repres', 'ar']\n",
            "['perceiv', 'perceiv']\n",
            "['april', 'april', 'ent', 'consequ']\n",
            "[]\n",
            "['search']\n",
            "['research', 'in', 'research']\n",
            "['e.', 'e.']\n",
            "['effect', 'of', 'on', 'outcom']\n",
            "['journ', 'of']\n",
            "[]\n",
            "['baron', 'baron', 'j.', 'f.', 'baron']\n",
            "[]\n",
            "['war', 'war', 'war', 'of', 'adminis-', 'in', 'evolv']\n",
            "['journ', 'of', 'of', 'journ']\n",
            "['barsad']\n",
            "['effect', 'effect', 'cont', 'on']\n",
            "[]\n",
            "['armel']\n",
            "['perceiv', 'support']\n",
            "['influ', 'of']\n",
            "['journ', 'of', 'journ']\n",
            "['bass', 'bass', 'm.', 'avolio', 'j.', 'jung', 'i.']\n",
            "['y.']\n",
            "['predict', 'assess']\n",
            "['apply', 'apply']\n",
            "['m.', 'm.']\n",
            "['s.']\n",
            "['in', 'for']\n",
            "[]\n",
            "['of', 'bed', 'bed', 'bed']\n",
            "['amnes', 'amnes', 'of']\n",
            "['lead']\n",
            "['–205']\n",
            "['m.', 'm.']\n",
            "['b.']\n",
            "['er', 'as', 'of']\n",
            "['view']\n",
            "['in', 'hartel', 'zerb']\n",
            "['in', 'emot', 'the']\n",
            "['westport', 'ct']\n",
            "['bergeron', 'bergeron']\n",
            "['paradox', 'of', 'paradox', 'at']\n",
            "['academy', 'of', '32']\n",
            "['atw']\n",
            "['mat']\n",
            "['psycholog']\n",
            "['avolio', 'avolio', 'j.']\n",
            "['m.']\n",
            "['consid', 'at', 'of', 'consid', 'examin', 'of']\n",
            "['quart', '–218']\n",
            "['j.', 'j.', 'j.', 'j.']\n",
            "['lead', 'lead', 'the', 'research']\n",
            "['in']\n",
            "['fisk', 'fisk']\n",
            "['e.', 'tayl', 'e.', 'of']\n",
            "[]\n",
            "[]\n",
            "['ca']\n",
            "['bacharach', 'bacharach', 'bamberg']\n",
            "[]\n",
            "['process', 'process', 'mi-', 'of', 'cropolit', 'align-', 'of', 'of']\n",
            "['sci']\n",
            "[]\n",
            "['477–506']\n",
            "['s.']\n",
            "['aid', 'aid', 'aid', 'of', 'of']\n",
            "['ithac', 'ithac']\n",
            "['bacharach', 'bacharach']\n",
            "['diver-', 'diver-', 'at']\n",
            "['rel', 'among']\n",
            "['academy', 'of']\n",
            "[]\n",
            "['bamberg']\n",
            "['seek']\n",
            "['exchang', 'exchang', 'in']\n",
            "['york', 'york', 'wiley']\n",
            "[]\n",
            "['soldy', 'soldy']\n",
            "['of', 'bolino', 'c.', 'bolino']\n",
            "['cost', 'of', 'cost', 'between', 'conscienty', 'conscienty', 'job', 'stress']\n",
            "['journ', 'of', 'journ', '–748', '–748']\n",
            "['emot', 'emot', 'in']\n",
            "['london']\n",
            "[]\n",
            "['bowl', 'bowl', 'm.', 'brass']\n",
            "['correl', 'of', 'correl']\n",
            "['journ', 'of']\n",
            "['70', 'v.', '70', 'e.']\n",
            "['get']\n",
            "['press', 'of', 'in']\n",
            "['–275']\n",
            "['cameron', 'cameron', 'k.', 'dutton', 'j.', 'quin', 'r.']\n",
            "[]\n",
            "['scholarship']\n",
            "['francisco', 'francisco']\n",
            "['cialdin']\n",
            "['b.', 'b.', 'j.']\n",
            "['proc', 'induc', 'toegel', 'kilduff', 'in']\n",
            "['of', 'person']\n",
            "[]\n",
            "['effect', 'of', 'on', 'interper-']\n",
            "['journ', 'of', 'person']\n",
            "['a.']\n",
            "['w.', 'w.', 'hei', 'b.', 'w.']\n",
            "['techn', 'of', 'ov', 'in']\n",
            "['sci']\n",
            "[]\n",
            "['botton']\n",
            "['pleas', 'pleas', 'of']\n",
            "['york', 'york']\n",
            "['good', 'of', 'at']\n",
            "[]\n",
            "['bulletin', 'bulletin']\n",
            "['gerstn', 'c.', 'gerstn']\n",
            "['correl', 'of', 'correl', 'struct']\n",
            "['journ', 'of']\n",
            "[]\n",
            "['gio', 'gio']\n",
            "['sensemak', 'in']\n",
            "['journ']\n",
            "['gio']\n",
            "['symbol', 'symbol', 'in', 'symbol', 'of', 'sensemak']\n",
            "[]\n",
            "['sci']\n",
            "['dyn', 'l.']\n",
            "[]\n",
            "[]\n",
            "['behavy', 'behavy', 'of']\n",
            "['academy', 'of', 'academy']\n",
            "['glas', 'glas', 'glas']\n",
            "['discovery', 'of', 'the', 'for']\n",
            "['chicago', 'chicago']\n",
            "['edward', 'edward']\n",
            "['terrain', 'terrain', 'of', 'in']\n",
            "['york']\n",
            "['goldstein', 'goldstein', 'n.']\n",
            "[]\n",
            "['us', 'in']\n",
            "['man', 'man']\n",
            "['eisenberg', 'eisenberg', 'karagonl', 'f.', 'nev', 'p.', 'eisenberg', 'mor', 'm.', 'muel']\n",
            "['exchang', 'exchang', 'exchang']\n",
            "['’s', 'of', '’s']\n",
            "['journ', 'of']\n",
            "['gouldn', 'gouldn']\n",
            "['norm', 'of', 'norm']\n",
            "['review']\n",
            "['eisenhardt', 'eisenhardt', 'k.', 'graebn']\n",
            "['build', 'from', 'build', 'opportun']\n",
            "['academy', 'of']\n",
            "[]\n",
            "['elfenbein', 'elfenbein']\n",
            "['emot', 'in']\n",
            "['in']\n",
            "['walsh', 'walsh']\n",
            "['brief', 'brief', 'brief', 'of']\n",
            "[]\n",
            "[]\n",
            "['york', 'york']\n",
            "['finem']\n",
            "[]\n",
            "['of']\n",
            "['flyn']\n",
            "['j.', 'j.', 'reag', 'e.', 'amanatullah', 't.', 'am']\n",
            "[]\n",
            "['’s', '’s', 'achiev', 'achiev', 'help', 'help', 'help']\n",
            "['frost', 'of', 'person']\n",
            "['at', 'handl', 'handl']\n",
            "['bos-']\n",
            "['ton', 'ton']\n",
            "['frost', 'frost', 'p.', 'dutton', 'j.', 'worlin', 'm.', 'frost']\n",
            "['nar', 'of', 'in']\n",
            "['in', 'in', 'finem', 'finem', 'in']\n",
            "['london']\n",
            "['frost', 'frost', 'p.']\n",
            "['handl', 'orga-', 'hero', 'handl']\n",
            "[]\n",
            "['view']\n",
            "[]\n",
            "['georg', 'georg', 'm.']\n",
            "[]\n",
            "['property', 'of']\n",
            "['journ', 'of']\n",
            "[]\n",
            "['grandey', 'grandey']\n",
            "['emot', 'emot']\n",
            "['plac', 'plac', 'conceiv']\n",
            "['journ', 'of']\n",
            "[]\n",
            "['grandey', 'grandey']\n",
            "['go', 'as', 'of', 'exhaust']\n",
            "[]\n",
            "['emy', 'of', 'emy', '–96', 'grandey']\n",
            "['emot', 'at', 'emot', 'review']\n",
            "['in', 'coop', 'coop']\n",
            "[]\n",
            "['handbook', 'of', 'handbook', 'oak', 'sag', 'rafael']\n",
            "['cyc', 'on', 'of', 'in']\n",
            "['in']\n",
            "['p.']\n",
            "['brief', 'brief']\n",
            "['staw', 'staw', 'staw', 'in', 'research']\n",
            "[]\n",
            "['jai']\n",
            "['hochschild']\n",
            "['r.']\n",
            "['heart', 'heart', 'commer-', 'of']\n",
            "['berkeley']\n",
            "['univers', 'of']\n",
            "['howel', 'howel', 'm.']\n",
            "['j.']\n",
            "['study', 'of']\n",
            "['behavy', 'behavy']\n",
            "[]\n",
            "['humphrey', 'humphrey']\n",
            "[]\n",
            "['fac', 'of']\n",
            "[]\n",
            "[]\n",
            "['of']\n",
            "['emot', 'facilit', 'chang', 'balanc-']\n",
            "[]\n",
            "['e.']\n",
            "['hartel', 'hartel', 'zerb', 'hartel', 'ashkanasy', 'hartel', 'in']\n",
            "[]\n",
            "['kahn', 'kahn']\n",
            "[]\n",
            "['for', 'of']\n",
            "['sci']\n",
            "['–563']\n",
            "['katz', 'katz', 'd.', 'katz']\n",
            "['l.']\n",
            "['psycholog', 'of']\n",
            "['york', 'york']\n",
            "['krackhardt', 'krackhardt']\n",
            "['assess', 'landscap', 'structure', 'cognit', 'in']\n",
            "['342–369', '342–369', 'j.', 'j.']\n",
            "['emot', 'emot', 'in', 'emot']\n",
            "['in', 'in']\n",
            "[]\n",
            "['don']\n",
            "['sag']\n",
            "['latané', 'b.', 'latané']\n",
            "['ar', 'ar']\n",
            "['journ', 'of']\n",
            "['47', '–327', 'lawl', 'e.', '–327']\n",
            "['bring', 'into']\n",
            "['in', 'cook', 'cook', 'of']\n",
            "[]\n",
            "[]\n",
            "['review']\n",
            "['org']\n",
            "['behav-']\n",
            "['syndrom']\n",
            "['ma', 'ma']\n",
            "['org']\n",
            "['construct']\n",
            "['perform']\n",
            "['pel']\n",
            "['psycholog', 'of', 'psycholog', 'the']\n",
            "['york', 'york']\n",
            "['podsakoff']\n",
            "['p.']\n",
            "['w.', 'w.']\n",
            "['d.']\n",
            "['consequ', 'of']\n",
            "['havy', 'havy']\n",
            "['journ', 'of', 'of']\n",
            "['rafferty', 'rafferty', 'e.', 'griffin']\n",
            "['definit', 'of', 'de-', 'lead']\n",
            "['richard', 'of', 'of']\n",
            "['us', 'in']\n",
            "['london']\n",
            "['rimé']\n",
            "['reg']\n",
            "['in', 'j.', 'j.', 'of']\n",
            "[]\n",
            "[]\n",
            "['york', 'york', 'guilford', 'lawl', 'e.', 'thy', 's.', 'york']\n",
            "['ment', 'in']\n",
            "['york', 'york']\n",
            "['roethlisberg', 'f.']\n",
            "['man', 'man', 'work', 'of', 'by', 'company']\n",
            "['ma', 'ma']\n",
            "['low', 'low', 'kroeck', 'sivasubramaniam']\n",
            "['correl', 'of', 'correl', 'of']\n",
            "[]\n",
            "[]\n",
            "['distort', 'psycholog', 'in']\n",
            "['in', 'in']\n",
            "[]\n",
            "['york', 'york', 'martin', 'j.', 'knopoff', 'k.']\n",
            "['tiv', 'to', 'imperson']\n",
            "['bound', 'at']\n",
            "[]\n",
            "[]\n",
            "['meyerson', 'meyerson']\n",
            "['hon']\n",
            "['in', 'finem', 'in']\n",
            "['london']\n",
            "['mil', 'mil', 'm.', 'mil']\n",
            "['anal-']\n",
            "['sag']\n",
            "['mintzberg']\n",
            "['nat', 'of']\n",
            "['york', 'york']\n",
            "['e.']\n",
            "['definit', 'definit', 'definit']\n",
            "['perspect']\n",
            "['of']\n",
            "['oldham', 'oldham', 'r.', 'brass']\n",
            "['react', 'to', 'off']\n",
            "[]\n",
            "['rousseau', 'rousseau']\n",
            "['contract', 'in']\n",
            "['right', 'right', 'right']\n",
            "['rousseau', 'rousseau']\n",
            "['contract', 'in', 'understand']\n",
            "['london', 'london']\n",
            "['settoon']\n",
            "['p.', 'p.']\n",
            "['w.']\n",
            "['context', 'as', 'of', 'cit']\n",
            "['journ', 'of']\n",
            "[]\n",
            "['simon', 'simon', 'r.', 'nath']\n",
            "['gend', 'gend', 'in']\n",
            "['diff', 'men', 'in', 'of', 'feel']\n",
            "['journ', 'of']\n",
            "['spect', 'spect', 'e.']\n",
            "[]\n",
            "['about', 'the', 'cit', 'of', 'of', 'cit']\n",
            "['man-']\n",
            "['review', 'review', 'toegel', 'kilduff', 'anand', 'spitzmul', 'toegel', 'dyn', 'l.', 'toegel']\n",
            "[]\n",
            "['behavy', 'behavy', 'review', 'of']\n",
            "['in', 'coop', 'barl', 'havy', 'of']\n",
            "['london']\n",
            "[]\n",
            "['interview']\n",
            "['york', 'york', 'rinehart']\n",
            "['sutton', 'sutton', 'i.', 'rafael']\n",
            "['charact', 'of', 'charact', 'as']\n",
            "[]\n",
            "['of']\n",
            "['thal', 'thal']\n",
            "['mat']\n",
            "['journ', 'of']\n",
            "['thoit']\n",
            "['sociolog', 'of']\n",
            "['in']\n",
            "['scot', 'blak', 'blak', 'scot', 'of']\n",
            "[]\n",
            "['ca']\n",
            "['g.', 'g.']\n",
            "['emot', 'emot', 'emot', 'of', 'affect']\n",
            "['psycholog']\n",
            "['tsu', 'tsu', 's.', 'pearc', 'l.', 'port', 'w.', 'tripol']\n",
            "[]\n",
            "['to', 'pay', 'in']\n",
            "['academy', 'of']\n",
            "['–1121']\n",
            "['turn']\n",
            "['rol', 'rol']\n",
            "['journ', 'of', 'journ', 'dyn', 'l.', 'cum', 'l.', 'journ']\n",
            "['behavy', 'in', 'of', 'construct']\n",
            "['in', 'cum', 'cum', 'staw', 'in', 'in']\n",
            "['stamford', 'ct']\n",
            "['dyn', 'dyn', 'l.', 'dyn']\n",
            "[]\n",
            "['creep']\n",
            "['be-', 'on', 'as', 'of']\n",
            "['in', 'm.', 'shapiro', 'shor', 'shapiro', 'm.', 'tetrick', 'amin', 'amin']\n",
            "['univers']\n",
            "['maan', 'maan']\n",
            "['tal', 'of', 'writ']\n",
            "['chicago', 'chicago', 'of']\n",
            "['wayn', 'wayn', 's.', 'shor', 'lid']\n",
            "['perceiv', 'support', 'support']\n",
            "['journ']\n",
            "['william', 'william', 'j.', 'william']\n",
            "['bound', 'in', 'rel', 'cas']\n",
            "['journ', 'journ', '–311', 'zell', 'l.']\n",
            "['person', 'person', 'of', 'in']\n",
            "['journ', 'of']\n",
            "['appendix', 'appendix']\n",
            "['try', 'us', 'us', 'of', 'with']\n",
            "['ask', 'ask', 'of', 'expery', 'expery', 'stress', 'anxi-', 'ety', 'tend', 'anxi-', 'of', 'cop', 'with']\n",
            "['describ', 'describ']\n",
            "[]\n",
            "['was', 'of', 'was']\n",
            "['provid', 'in', 'provid', 'of', 'kind']\n",
            "[]\n",
            "['provid', 'nee']\n",
            "['mad', 'feel']\n",
            "['think']\n",
            "['help', 'nee']\n",
            "[]\n",
            "['wer', 'wer']\n",
            "[]\n",
            "['ment', 'ar', 'provid']\n",
            "['is', 'is', 'help', 'of']\n",
            "['pay']\n",
            "[]\n",
            "['help']\n",
            "['in']\n",
            "[]\n",
            "['think', 'of', 'as']\n",
            "['toegel', 'is', 'of', 'at']\n",
            "['receiv', 'receiv']\n",
            "['in', 'from', 'univers']\n",
            "['in', 'from']\n",
            "['foc', 'on', 'in', 'interest', 'believ']\n",
            "['is', 'is', 'of', 'of', 'at']\n",
            "['receiv', 'receiv']\n",
            "['from']\n",
            "['includ', 'research', 'includ', 'network', 'emot', 'per-', 'son', 'of']\n",
            "['anand', 'anand', 'is', 'of', 'at']\n",
            "['receiv', 'receiv']\n",
            "['from']\n",
            "['focus', 'on', 'in', 'firm', 'form', 'of', 'in']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "14WEYZaSd9H9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import ast"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tAZXIsjkP6Pk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "47372409-4b7c-4001-ed04-81317da47591"
      },
      "cell_type": "code",
      "source": [
        "question= en_nlp(question_now.lower()).sents\n",
        "question_root = st.stem(str([sent.root for sent in question][0]))\n",
        "# print(question_root)\n",
        "\n",
        "\n",
        "# for i,sent in enumerate(question_now):\n",
        "#         question_root = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n",
        "# # print(question_root)\n",
        "\n",
        "li = []\n",
        "sentences = en_nlp(doc3.lower()).sents\n",
        "# for i,sent in enumerate(sentences):\n",
        "#   print(sent)\n",
        "for i,sent in enumerate(sentences):\n",
        "    roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n",
        "#     print(roots)\n",
        "    for temp_root in roots:\n",
        "#       print(temp_root)\n",
        "      if question_root == temp_root:\n",
        "#         print('roots:' + str(roots))\n",
        "#         print('question_root:' + str(question_root))\n",
        "        print('sentence:' + str(sent))\n",
        "#         for k,item in enumerate(ast.literal_eval(sentences)):\n",
        "#           if str(sent) in item.lower(): \n",
        "#             li.append(k)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence:some\n",
            "managers were unavailable for interviews, because\n",
            "they were involved in starting up a branch of the\n",
            "firm in another city.\n",
            "sentence:so\n",
            "they start telling others, “don’t do it the [company]\n",
            "way, do it this way.\n",
            "sentence:i have helped them [and enabled them to]\n",
            "focus [on their task], and if they start achieving\n",
            "[their job goals], what delights me is the recognition:\n",
            "recognition in the sense that they know that i\n",
            "helped them.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mMwtk-_7QkND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def match_roots(x):\n",
        "#     question = x[\"question\"].lower()\n",
        "#     sentences = en_nlp(x[\"context\"].lower()).sents\n",
        "    \n",
        "#     question_root = st.stem(str([sent.root for sent in en_nlp(question).sents][0]))\n",
        "    \n",
        "#     li = []\n",
        "#     for i,sent in enumerate(sentences):\n",
        "#         roots = [st.stem(chunk.root.head.text.lower()) for chunk in sent.noun_chunks]\n",
        "\n",
        "#         if question_root in roots: \n",
        "#             for k,item in enumerate(ast.literal_eval(x[\"sentences\"])):\n",
        "#                 if str(sent) in item.lower(): \n",
        "#                     li.append(k)\n",
        "#     return li"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}